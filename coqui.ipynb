{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Xtts(\n",
       "  (gpt): GPT(\n",
       "    (conditioning_encoder): ConditioningEncoder(\n",
       "      (init): Conv1d(80, 1024, kernel_size=(1,), stride=(1,))\n",
       "      (attn): Sequential(\n",
       "        (0): AttentionBlock(\n",
       "          (norm): GroupNorm32(32, 1024, eps=1e-05, affine=True)\n",
       "          (qkv): Conv1d(1024, 3072, kernel_size=(1,), stride=(1,))\n",
       "          (attention): QKVAttention()\n",
       "          (x_proj): Identity()\n",
       "          (proj_out): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (1): AttentionBlock(\n",
       "          (norm): GroupNorm32(32, 1024, eps=1e-05, affine=True)\n",
       "          (qkv): Conv1d(1024, 3072, kernel_size=(1,), stride=(1,))\n",
       "          (attention): QKVAttention()\n",
       "          (x_proj): Identity()\n",
       "          (proj_out): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (2): AttentionBlock(\n",
       "          (norm): GroupNorm32(32, 1024, eps=1e-05, affine=True)\n",
       "          (qkv): Conv1d(1024, 3072, kernel_size=(1,), stride=(1,))\n",
       "          (attention): QKVAttention()\n",
       "          (x_proj): Identity()\n",
       "          (proj_out): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (3): AttentionBlock(\n",
       "          (norm): GroupNorm32(32, 1024, eps=1e-05, affine=True)\n",
       "          (qkv): Conv1d(1024, 3072, kernel_size=(1,), stride=(1,))\n",
       "          (attention): QKVAttention()\n",
       "          (x_proj): Identity()\n",
       "          (proj_out): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (4): AttentionBlock(\n",
       "          (norm): GroupNorm32(32, 1024, eps=1e-05, affine=True)\n",
       "          (qkv): Conv1d(1024, 3072, kernel_size=(1,), stride=(1,))\n",
       "          (attention): QKVAttention()\n",
       "          (x_proj): Identity()\n",
       "          (proj_out): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (5): AttentionBlock(\n",
       "          (norm): GroupNorm32(32, 1024, eps=1e-05, affine=True)\n",
       "          (qkv): Conv1d(1024, 3072, kernel_size=(1,), stride=(1,))\n",
       "          (attention): QKVAttention()\n",
       "          (x_proj): Identity()\n",
       "          (proj_out): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (conditioning_dropout): Dropout1d(p=0.1, inplace=False)\n",
       "    (text_embedding): Embedding(6681, 1024)\n",
       "    (mel_embedding): Embedding(1026, 1024)\n",
       "    (gpt): GPT2Model(\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (h): ModuleList(\n",
       "        (0-29): 30 x GPT2Block(\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (wte): Embedding(1026, 1024)\n",
       "    )\n",
       "    (mel_pos_embedding): LearnedPositionEmbeddings(\n",
       "      (emb): Embedding(608, 1024)\n",
       "    )\n",
       "    (text_pos_embedding): LearnedPositionEmbeddings(\n",
       "      (emb): Embedding(404, 1024)\n",
       "    )\n",
       "    (final_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (text_head): Linear(in_features=1024, out_features=6681, bias=True)\n",
       "    (mel_head): Linear(in_features=1024, out_features=1026, bias=True)\n",
       "    (conditioning_perceiver): PerceiverResampler(\n",
       "      (proj_context): Identity()\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x ModuleList(\n",
       "          (0): Attention(\n",
       "            (attend): Attend(\n",
       "              (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (to_q): Linear(in_features=1024, out_features=512, bias=False)\n",
       "            (to_kv): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (to_out): Linear(in_features=512, out_features=1024, bias=False)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=5460, bias=True)\n",
       "            (1): GEGLU()\n",
       "            (2): Linear(in_features=2730, out_features=1024, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm): RMSNorm()\n",
       "    )\n",
       "    (gpt_inference): GPT2InferenceModel(\n",
       "      (transformer): GPT2Model(\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "        (h): ModuleList(\n",
       "          (0-29): 30 x GPT2Block(\n",
       "            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPT2Attention(\n",
       "              (c_attn): Conv1D()\n",
       "              (c_proj): Conv1D()\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): GPT2MLP(\n",
       "              (c_fc): Conv1D()\n",
       "              (c_proj): Conv1D()\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (wte): Embedding(1026, 1024)\n",
       "      )\n",
       "      (pos_embedding): LearnedPositionEmbeddings(\n",
       "        (emb): Embedding(608, 1024)\n",
       "      )\n",
       "      (embeddings): Embedding(1026, 1024)\n",
       "      (final_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (lm_head): Sequential(\n",
       "        (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): Linear(in_features=1024, out_features=1026, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (hifigan_decoder): HifiDecoder(\n",
       "    (waveform_decoder): HifiganGenerator(\n",
       "      (conv_pre): Conv1d(1024, 512, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "      (ups): ModuleList(\n",
       "        (0): ParametrizedConvTranspose1d(\n",
       "          512, 256, kernel_size=(16,), stride=(8,), padding=(4,)\n",
       "          (parametrizations): ModuleDict(\n",
       "            (weight): ParametrizationList(\n",
       "              (0): _WeightNorm()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): ParametrizedConvTranspose1d(\n",
       "          256, 128, kernel_size=(16,), stride=(8,), padding=(4,)\n",
       "          (parametrizations): ModuleDict(\n",
       "            (weight): ParametrizationList(\n",
       "              (0): _WeightNorm()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): ParametrizedConvTranspose1d(\n",
       "          128, 64, kernel_size=(4,), stride=(2,), padding=(1,)\n",
       "          (parametrizations): ModuleDict(\n",
       "            (weight): ParametrizationList(\n",
       "              (0): _WeightNorm()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): ParametrizedConvTranspose1d(\n",
       "          64, 32, kernel_size=(4,), stride=(2,), padding=(1,)\n",
       "          (parametrizations): ModuleDict(\n",
       "            (weight): ParametrizationList(\n",
       "              (0): _WeightNorm()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (resblocks): ModuleList(\n",
       "        (0): ResBlock1(\n",
       "          (convs1): ModuleList(\n",
       "            (0): ParametrizedConv1d(\n",
       "              256, 256, kernel_size=(3,), stride=(1,), padding=(1,)\n",
       "              (parametrizations): ModuleDict(\n",
       "                (weight): ParametrizationList(\n",
       "                  (0): _WeightNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (1): ParametrizedConv1d(\n",
       "              256, 256, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,)\n",
       "              (parametrizations): ModuleDict(\n",
       "                (weight): ParametrizationList(\n",
       "                  (0): _WeightNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (2): ParametrizedConv1d(\n",
       "              256, 256, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,)\n",
       "              (parametrizations): ModuleDict(\n",
       "                (weight): ParametrizationList(\n",
       "                  (0): _WeightNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (convs2): ModuleList(\n",
       "            (0-2): 3 x ParametrizedConv1d(\n",
       "              256, 256, kernel_size=(3,), stride=(1,), padding=(1,)\n",
       "              (parametrizations): ModuleDict(\n",
       "                (weight): ParametrizationList(\n",
       "                  (0): _WeightNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): ResBlock1(\n",
       "          (convs1): ModuleList(\n",
       "            (0): ParametrizedConv1d(\n",
       "              256, 256, kernel_size=(7,), stride=(1,), padding=(3,)\n",
       "              (parametrizations): ModuleDict(\n",
       "                (weight): ParametrizationList(\n",
       "                  (0): _WeightNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (1): ParametrizedConv1d(\n",
       "              256, 256, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,)\n",
       "              (parametrizations): ModuleDict(\n",
       "                (weight): ParametrizationList(\n",
       "                  (0): _WeightNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (2): ParametrizedConv1d(\n",
       "              256, 256, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,)\n",
       "              (parametrizations): ModuleDict(\n",
       "                (weight): ParametrizationList(\n",
       "                  (0): _WeightNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (convs2): ModuleList(\n",
       "            (0-2): 3 x ParametrizedConv1d(\n",
       "              256, 256, kernel_size=(7,), stride=(1,), padding=(3,)\n",
       "              (parametrizations): ModuleDict(\n",
       "                (weight): ParametrizationList(\n",
       "                  (0): _WeightNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): ResBlock1(\n",
       "          (convs1): ModuleList(\n",
       "            (0): ParametrizedConv1d(\n",
       "              256, 256, kernel_size=(11,), stride=(1,), padding=(5,)\n",
       "              (parametrizations): ModuleDict(\n",
       "                (weight): ParametrizationList(\n",
       "                  (0): _WeightNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (1): ParametrizedConv1d(\n",
       "              256, 256, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,)\n",
       "              (parametrizations): ModuleDict(\n",
       "                (weight): ParametrizationList(\n",
       "                  (0): _WeightNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (2): ParametrizedConv1d(\n",
       "              256, 256, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,)\n",
       "              (parametrizations): ModuleDict(\n",
       "                (weight): ParametrizationList(\n",
       "                  (0): _WeightNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (convs2): ModuleList(\n",
       "            (0-2): 3 x ParametrizedConv1d(\n",
       "              256, 256, kernel_size=(11,), stride=(1,), padding=(5,)\n",
       "              (parametrizations): ModuleDict(\n",
       "                (weight): ParametrizationList(\n",
       "                  (0): _WeightNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): ResBlock1(\n",
       "          (convs1): ModuleList(\n",
       "            (0): ParametrizedConv1d(\n",
       "              128, 128, kernel_size=(3,), stride=(1,), padding=(1,)\n",
       "              (parametrizations): ModuleDict(\n",
       "                (weight): ParametrizationList(\n",
       "                  (0): _WeightNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (1): ParametrizedConv1d(\n",
       "              128, 128, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,)\n",
       "              (parametrizations): ModuleDict(\n",
       "                (weight): ParametrizationList(\n",
       "                  (0): _WeightNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (2): ParametrizedConv1d(\n",
       "              128, 128, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,)\n",
       "              (parametrizations): ModuleDict(\n",
       "                (weight): ParametrizationList(\n",
       "                  (0): _WeightNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (convs2): ModuleList(\n",
       "            (0-2): 3 x ParametrizedConv1d(\n",
       "              128, 128, kernel_size=(3,), stride=(1,), padding=(1,)\n",
       "              (parametrizations): ModuleDict(\n",
       "                (weight): ParametrizationList(\n",
       "                  (0): _WeightNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (4): ResBlock1(\n",
       "          (convs1): ModuleList(\n",
       "            (0): ParametrizedConv1d(\n",
       "              128, 128, kernel_size=(7,), stride=(1,), padding=(3,)\n",
       "              (parametrizations): ModuleDict(\n",
       "                (weight): ParametrizationList(\n",
       "                  (0): _WeightNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (1): ParametrizedConv1d(\n",
       "              128, 128, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,)\n",
       "              (parametrizations): ModuleDict(\n",
       "                (weight): ParametrizationList(\n",
       "                  (0): _WeightNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (2): ParametrizedConv1d(\n",
       "              128, 128, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,)\n",
       "              (parametrizations): ModuleDict(\n",
       "                (weight): ParametrizationList(\n",
       "                  (0): _WeightNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (convs2): ModuleList(\n",
       "            (0-2): 3 x ParametrizedConv1d(\n",
       "              128, 128, kernel_size=(7,), stride=(1,), padding=(3,)\n",
       "              (parametrizations): ModuleDict(\n",
       "                (weight): ParametrizationList(\n",
       "                  (0): _WeightNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (5): ResBlock1(\n",
       "          (convs1): ModuleList(\n",
       "            (0): ParametrizedConv1d(\n",
       "              128, 128, kernel_size=(11,), stride=(1,), padding=(5,)\n",
       "              (parametrizations): ModuleDict(\n",
       "                (weight): ParametrizationList(\n",
       "                  (0): _WeightNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (1): ParametrizedConv1d(\n",
       "              128, 128, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,)\n",
       "              (parametrizations): ModuleDict(\n",
       "                (weight): ParametrizationList(\n",
       "                  (0): _WeightNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (2): ParametrizedConv1d(\n",
       "              128, 128, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,)\n",
       "              (parametrizations): ModuleDict(\n",
       "                (weight): ParametrizationList(\n",
       "                  (0): _WeightNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (convs2): ModuleList(\n",
       "            (0-2): 3 x ParametrizedConv1d(\n",
       "              128, 128, kernel_size=(11,), stride=(1,), padding=(5,)\n",
       "              (parametrizations): ModuleDict(\n",
       "                (weight): ParametrizationList(\n",
       "                  (0): _WeightNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (6): ResBlock1(\n",
       "          (convs1): ModuleList(\n",
       "            (0): ParametrizedConv1d(\n",
       "              64, 64, kernel_size=(3,), stride=(1,), padding=(1,)\n",
       "              (parametrizations): ModuleDict(\n",
       "                (weight): ParametrizationList(\n",
       "                  (0): _WeightNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (1): ParametrizedConv1d(\n",
       "              64, 64, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,)\n",
       "              (parametrizations): ModuleDict(\n",
       "                (weight): ParametrizationList(\n",
       "                  (0): _WeightNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (2): ParametrizedConv1d(\n",
       "              64, 64, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,)\n",
       "              (parametrizations): ModuleDict(\n",
       "                (weight): ParametrizationList(\n",
       "                  (0): _WeightNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (convs2): ModuleList(\n",
       "            (0-2): 3 x ParametrizedConv1d(\n",
       "              64, 64, kernel_size=(3,), stride=(1,), padding=(1,)\n",
       "              (parametrizations): ModuleDict(\n",
       "                (weight): ParametrizationList(\n",
       "                  (0): _WeightNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (7): ResBlock1(\n",
       "          (convs1): ModuleList(\n",
       "            (0): ParametrizedConv1d(\n",
       "              64, 64, kernel_size=(7,), stride=(1,), padding=(3,)\n",
       "              (parametrizations): ModuleDict(\n",
       "                (weight): ParametrizationList(\n",
       "                  (0): _WeightNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (1): ParametrizedConv1d(\n",
       "              64, 64, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,)\n",
       "              (parametrizations): ModuleDict(\n",
       "                (weight): ParametrizationList(\n",
       "                  (0): _WeightNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (2): ParametrizedConv1d(\n",
       "              64, 64, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,)\n",
       "              (parametrizations): ModuleDict(\n",
       "                (weight): ParametrizationList(\n",
       "                  (0): _WeightNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (convs2): ModuleList(\n",
       "            (0-2): 3 x ParametrizedConv1d(\n",
       "              64, 64, kernel_size=(7,), stride=(1,), padding=(3,)\n",
       "              (parametrizations): ModuleDict(\n",
       "                (weight): ParametrizationList(\n",
       "                  (0): _WeightNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (8): ResBlock1(\n",
       "          (convs1): ModuleList(\n",
       "            (0): ParametrizedConv1d(\n",
       "              64, 64, kernel_size=(11,), stride=(1,), padding=(5,)\n",
       "              (parametrizations): ModuleDict(\n",
       "                (weight): ParametrizationList(\n",
       "                  (0): _WeightNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (1): ParametrizedConv1d(\n",
       "              64, 64, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,)\n",
       "              (parametrizations): ModuleDict(\n",
       "                (weight): ParametrizationList(\n",
       "                  (0): _WeightNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (2): ParametrizedConv1d(\n",
       "              64, 64, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,)\n",
       "              (parametrizations): ModuleDict(\n",
       "                (weight): ParametrizationList(\n",
       "                  (0): _WeightNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (convs2): ModuleList(\n",
       "            (0-2): 3 x ParametrizedConv1d(\n",
       "              64, 64, kernel_size=(11,), stride=(1,), padding=(5,)\n",
       "              (parametrizations): ModuleDict(\n",
       "                (weight): ParametrizationList(\n",
       "                  (0): _WeightNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (9): ResBlock1(\n",
       "          (convs1): ModuleList(\n",
       "            (0): ParametrizedConv1d(\n",
       "              32, 32, kernel_size=(3,), stride=(1,), padding=(1,)\n",
       "              (parametrizations): ModuleDict(\n",
       "                (weight): ParametrizationList(\n",
       "                  (0): _WeightNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (1): ParametrizedConv1d(\n",
       "              32, 32, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,)\n",
       "              (parametrizations): ModuleDict(\n",
       "                (weight): ParametrizationList(\n",
       "                  (0): _WeightNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (2): ParametrizedConv1d(\n",
       "              32, 32, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,)\n",
       "              (parametrizations): ModuleDict(\n",
       "                (weight): ParametrizationList(\n",
       "                  (0): _WeightNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (convs2): ModuleList(\n",
       "            (0-2): 3 x ParametrizedConv1d(\n",
       "              32, 32, kernel_size=(3,), stride=(1,), padding=(1,)\n",
       "              (parametrizations): ModuleDict(\n",
       "                (weight): ParametrizationList(\n",
       "                  (0): _WeightNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (10): ResBlock1(\n",
       "          (convs1): ModuleList(\n",
       "            (0): ParametrizedConv1d(\n",
       "              32, 32, kernel_size=(7,), stride=(1,), padding=(3,)\n",
       "              (parametrizations): ModuleDict(\n",
       "                (weight): ParametrizationList(\n",
       "                  (0): _WeightNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (1): ParametrizedConv1d(\n",
       "              32, 32, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,)\n",
       "              (parametrizations): ModuleDict(\n",
       "                (weight): ParametrizationList(\n",
       "                  (0): _WeightNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (2): ParametrizedConv1d(\n",
       "              32, 32, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,)\n",
       "              (parametrizations): ModuleDict(\n",
       "                (weight): ParametrizationList(\n",
       "                  (0): _WeightNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (convs2): ModuleList(\n",
       "            (0-2): 3 x ParametrizedConv1d(\n",
       "              32, 32, kernel_size=(7,), stride=(1,), padding=(3,)\n",
       "              (parametrizations): ModuleDict(\n",
       "                (weight): ParametrizationList(\n",
       "                  (0): _WeightNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (11): ResBlock1(\n",
       "          (convs1): ModuleList(\n",
       "            (0): ParametrizedConv1d(\n",
       "              32, 32, kernel_size=(11,), stride=(1,), padding=(5,)\n",
       "              (parametrizations): ModuleDict(\n",
       "                (weight): ParametrizationList(\n",
       "                  (0): _WeightNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (1): ParametrizedConv1d(\n",
       "              32, 32, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,)\n",
       "              (parametrizations): ModuleDict(\n",
       "                (weight): ParametrizationList(\n",
       "                  (0): _WeightNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (2): ParametrizedConv1d(\n",
       "              32, 32, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,)\n",
       "              (parametrizations): ModuleDict(\n",
       "                (weight): ParametrizationList(\n",
       "                  (0): _WeightNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (convs2): ModuleList(\n",
       "            (0-2): 3 x ParametrizedConv1d(\n",
       "              32, 32, kernel_size=(11,), stride=(1,), padding=(5,)\n",
       "              (parametrizations): ModuleDict(\n",
       "                (weight): ParametrizationList(\n",
       "                  (0): _WeightNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (conv_post): Conv1d(32, 1, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n",
       "      (cond_layer): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "      (conds): ModuleList(\n",
       "        (0): Conv1d(512, 256, kernel_size=(1,), stride=(1,))\n",
       "        (1): Conv1d(512, 128, kernel_size=(1,), stride=(1,))\n",
       "        (2): Conv1d(512, 64, kernel_size=(1,), stride=(1,))\n",
       "        (3): Conv1d(512, 32, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "    )\n",
       "    (speaker_encoder): ResNetSpeakerEncoder(\n",
       "      (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (layer1): Sequential(\n",
       "        (0): SEBasicBlock(\n",
       "          (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (se): SELayer(\n",
       "            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc): Sequential(\n",
       "              (0): Linear(in_features=32, out_features=4, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=4, out_features=32, bias=True)\n",
       "              (3): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): SEBasicBlock(\n",
       "          (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (se): SELayer(\n",
       "            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc): Sequential(\n",
       "              (0): Linear(in_features=32, out_features=4, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=4, out_features=32, bias=True)\n",
       "              (3): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): SEBasicBlock(\n",
       "          (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (se): SELayer(\n",
       "            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc): Sequential(\n",
       "              (0): Linear(in_features=32, out_features=4, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=4, out_features=32, bias=True)\n",
       "              (3): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): SEBasicBlock(\n",
       "          (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (se): SELayer(\n",
       "            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc): Sequential(\n",
       "              (0): Linear(in_features=64, out_features=8, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=8, out_features=64, bias=True)\n",
       "              (3): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): SEBasicBlock(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (se): SELayer(\n",
       "            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc): Sequential(\n",
       "              (0): Linear(in_features=64, out_features=8, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=8, out_features=64, bias=True)\n",
       "              (3): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): SEBasicBlock(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (se): SELayer(\n",
       "            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc): Sequential(\n",
       "              (0): Linear(in_features=64, out_features=8, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=8, out_features=64, bias=True)\n",
       "              (3): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): SEBasicBlock(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (se): SELayer(\n",
       "            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc): Sequential(\n",
       "              (0): Linear(in_features=64, out_features=8, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=8, out_features=64, bias=True)\n",
       "              (3): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): SEBasicBlock(\n",
       "          (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (se): SELayer(\n",
       "            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc): Sequential(\n",
       "              (0): Linear(in_features=128, out_features=16, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=16, out_features=128, bias=True)\n",
       "              (3): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): SEBasicBlock(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (se): SELayer(\n",
       "            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc): Sequential(\n",
       "              (0): Linear(in_features=128, out_features=16, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=16, out_features=128, bias=True)\n",
       "              (3): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): SEBasicBlock(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (se): SELayer(\n",
       "            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc): Sequential(\n",
       "              (0): Linear(in_features=128, out_features=16, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=16, out_features=128, bias=True)\n",
       "              (3): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): SEBasicBlock(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (se): SELayer(\n",
       "            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc): Sequential(\n",
       "              (0): Linear(in_features=128, out_features=16, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=16, out_features=128, bias=True)\n",
       "              (3): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (4): SEBasicBlock(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (se): SELayer(\n",
       "            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc): Sequential(\n",
       "              (0): Linear(in_features=128, out_features=16, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=16, out_features=128, bias=True)\n",
       "              (3): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (5): SEBasicBlock(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (se): SELayer(\n",
       "            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc): Sequential(\n",
       "              (0): Linear(in_features=128, out_features=16, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=16, out_features=128, bias=True)\n",
       "              (3): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): SEBasicBlock(\n",
       "          (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (se): SELayer(\n",
       "            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc): Sequential(\n",
       "              (0): Linear(in_features=256, out_features=32, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=32, out_features=256, bias=True)\n",
       "              (3): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): SEBasicBlock(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (se): SELayer(\n",
       "            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc): Sequential(\n",
       "              (0): Linear(in_features=256, out_features=32, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=32, out_features=256, bias=True)\n",
       "              (3): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): SEBasicBlock(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (se): SELayer(\n",
       "            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc): Sequential(\n",
       "              (0): Linear(in_features=256, out_features=32, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=32, out_features=256, bias=True)\n",
       "              (3): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (instancenorm): InstanceNorm1d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (torch_spec): Sequential(\n",
       "        (0): PreEmphasis()\n",
       "        (1): MelSpectrogram(\n",
       "          (spectrogram): Spectrogram()\n",
       "          (mel_scale): MelScale()\n",
       "        )\n",
       "      )\n",
       "      (attention): Sequential(\n",
       "        (0): Conv1d(2048, 128, kernel_size=(1,), stride=(1,))\n",
       "        (1): ReLU()\n",
       "        (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): Conv1d(128, 2048, kernel_size=(1,), stride=(1,))\n",
       "        (4): Softmax(dim=2)\n",
       "      )\n",
       "      (fc): Linear(in_features=4096, out_features=512, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydub import AudioSegment\n",
    "import soundfile as sf\n",
    "from TTS.tts.configs.xtts_config import XttsConfig\n",
    "from TTS.tts.models.xtts import Xtts\n",
    "from IPython.display import Audio, display\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Load the TTS model\n",
    "config = XttsConfig()\n",
    "config.load_json(\"XTTS-v2\\\\config.json\")\n",
    "model = Xtts.init_from_config(config)\n",
    "model.load_checkpoint(config, checkpoint_dir=\"XTTS-v2\", eval=True, vocab_path=\"XTTS-v2\\\\vocab.json\")\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Hallo, mein Name ist Eren. Und, äh — und ich mag Pizza. Aber ich habe auch andere Interessen, wie zum Beispiel Tic Tac Toe zu spielen. \n",
    "Cuando no estoy leyendo, me gusta pasar tiempo al aire libre. Me encanta hacer senderismo en las montañas y explorar los senderos naturales. \n",
    "Une autre passion que j'ai est la technologie. Je suis toujours excité d'apprendre les dernières avancées en intelligence artificielle, en robotique et en exploration spatiale. \n",
    "Je crois que la technologie a le potentiel de résoudre de nombreux problèmes mondiaux et d'améliorer notre qualité de vie.\n",
    "In meiner Freizeit spiele ich auch gerne Videospiele. Einige meiner Lieblingsspiele sind Strategiespiele wie Civilization und Echtzeit-Strategiespiele wie StarCraft. \n",
    "Ich finde, dass diese Spiele meinen Geist herausfordern und mir helfen, strategisches Denken zu entwickeln. \n",
    "Außerdem machen sie viel Spaß, mit Freunden und Familie zu spielen.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kalin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\conv.py:306: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cudnn\\Conv_v8.cpp:919.)\n",
      "  return F.conv1d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Hallo.\n",
      "Average generation time per character: 2.2236289580663047 seconds\n",
      "Average generation time per token: 13.341773748397827 seconds\n",
      "Average audio duration per character: 2.291 seconds\n",
      "Average audio duration per token: 13.746 seconds\n",
      "\n",
      "Text: Ich mag Pizza.\n",
      "Average generation time per character: 0.13582447596958705 seconds\n",
      "Average generation time per token: 0.6338475545247396 seconds\n",
      "Average audio duration per character: 0.1617142857142857 seconds\n",
      "Average audio duration per token: 0.7546666666666666 seconds\n",
      "\n",
      "Text: Hola.\n",
      "Average generation time per character: 0.4366607189178467 seconds\n",
      "Average generation time per token: 2.1833035945892334 seconds\n",
      "Average audio duration per character: 0.5456000000000001 seconds\n",
      "Average audio duration per token: 2.728 seconds\n",
      "\n",
      "Text: Me gusta leer.\n",
      "Average generation time per character: 0.19866129330226354 seconds\n",
      "Average generation time per token: 0.9270860354105631 seconds\n",
      "Average audio duration per character: 0.25207142857142856 seconds\n",
      "Average audio duration per token: 1.1763333333333332 seconds\n",
      "\n",
      "Text: Bonjour.\n",
      "Average generation time per character: 0.3820926547050476 seconds\n",
      "Average generation time per token: 3.056741237640381 seconds\n",
      "Average audio duration per character: 0.49775 seconds\n",
      "Average audio duration per token: 3.982 seconds\n",
      "\n",
      "Text: J'aime la technologie.\n",
      "Average generation time per character: 0.07908084175803444 seconds\n",
      "Average generation time per token: 0.5799261728922526 seconds\n",
      "Average audio duration per character: 0.1029090909090909 seconds\n",
      "Average audio duration per token: 0.7546666666666666 seconds\n",
      "\n",
      "Text: Ciao.\n",
      "Average generation time per character: 0.28762102127075195 seconds\n",
      "Average generation time per token: 1.4381051063537598 seconds\n",
      "Average audio duration per character: 0.33199999999999996 seconds\n",
      "Average audio duration per token: 1.66 seconds\n",
      "\n",
      "Text: Mi piace la musica.\n",
      "Average generation time per character: 0.18249358628925524 seconds\n",
      "Average generation time per token: 0.8668445348739624 seconds\n",
      "Average audio duration per character: 0.242 seconds\n",
      "Average audio duration per token: 1.1495 seconds\n",
      "\n",
      "Text: こんにちは。\n",
      "Average generation time per character: 0.1842040220896403 seconds\n",
      "Average generation time per token: 1.1052241325378418 seconds\n",
      "Average audio duration per character: 0.18383333333333332 seconds\n",
      "Average audio duration per token: 1.103 seconds\n",
      "\n",
      "Text: 音楽が好きです。\n",
      "Average generation time per character: 0.1688278317451477 seconds\n",
      "Average generation time per token: 1.3506226539611816 seconds\n",
      "Average audio duration per character: 0.1945 seconds\n",
      "Average audio duration per token: 1.556 seconds\n",
      "\n",
      "Text: Hello.\n",
      "Average generation time per character: 0.18752539157867432 seconds\n",
      "Average generation time per token: 1.125152349472046 seconds\n",
      "Average audio duration per character: 0.2011666666666667 seconds\n",
      "Average audio duration per token: 1.207 seconds\n",
      "\n",
      "Text: I like reading.\n",
      "Average generation time per character: 0.061373297373453775 seconds\n",
      "Average generation time per token: 0.30686648686726886 seconds\n",
      "Average audio duration per character: 0.07353333333333333 seconds\n",
      "Average audio duration per token: 0.36766666666666664 seconds\n",
      "\n",
      "Text: Hallo, mein Name ist Suno. Und, äh — und ich mag Pizza.\n",
      "Average generation time per character: 0.15919128612235742 seconds\n",
      "Average generation time per token: 0.7163607875506083 seconds\n",
      "Average audio duration per character: 0.2177962962962963 seconds\n",
      "Average audio duration per token: 0.9800833333333333 seconds\n",
      "\n",
      "Text: Cuando no estoy leyendo, me gusta pasar tiempo al aire libre.\n",
      "Average generation time per character: 0.07850398391973777 seconds\n",
      "Average generation time per token: 0.43534027446400037 seconds\n",
      "Average audio duration per character: 0.10924590163934425 seconds\n",
      "Average audio duration per token: 0.6058181818181818 seconds\n",
      "\n",
      "Text: Une autre passion que j'ai est la technologie.\n",
      "Average generation time per character: 0.06662282218103824 seconds\n",
      "Average generation time per token: 0.38308122754096985 seconds\n",
      "Average audio duration per character: 0.08884782608695652 seconds\n",
      "Average audio duration per token: 0.510875 seconds\n",
      "\n",
      "Text: Quando si parla di esplorazione spaziale, sono un grande fan della NASA e di SpaceX.\n",
      "Average generation time per character: 0.06681059939520699 seconds\n",
      "Average generation time per token: 0.37413935661315917 seconds\n",
      "Average audio duration per character: 0.09260714285714286 seconds\n",
      "Average audio duration per token: 0.5186 seconds\n",
      "\n",
      "Text: 音楽は私の人生のもう一つの重要な部分です。\n",
      "Average generation time per character: 0.1161147639864967 seconds\n",
      "Average generation time per token: 2.4384100437164307 seconds\n",
      "Average audio duration per character: 0.14871428571428572 seconds\n",
      "Average audio duration per token: 3.123 seconds\n",
      "\n",
      "Text: Por ejemplo, las nueces de Brasil se llaman 'castañas de Brasil' en español, y son deliciosas.\n",
      "Average generation time per character: 0.05492892417501896 seconds\n",
      "Average generation time per token: 0.3227074295282364 seconds\n",
      "Average audio duration per character: 0.07681914893617021 seconds\n",
      "Average audio duration per token: 0.4513125 seconds\n",
      "\n",
      "Text: Hallo, mein Name ist Suno. Ich bin ein großer Fan von Pizza, besonders wenn sie mit frischen Zutaten und viel Käse zubereitet wird. In meiner Freizeit lese ich gerne Bücher, besonders Science-Fiction- und Fantasy-Romane. Einer meiner Lieblingsautoren ist Isaac Asimov, und ich liebe besonders seine Foundation-Serie. Die Art und Weise, wie er komplexe Erzählungen webt und die Zukunft der Menschheit erforscht, ist wirklich faszinierend.\n",
      "Average generation time per character: 0.0596452772479685 seconds\n",
      "Average generation time per token: 0.4035375788807869 seconds\n",
      "Average audio duration per character: 0.0821270207852194 seconds\n",
      "Average audio duration per token: 0.555640625 seconds\n",
      "\n",
      "Text: Cuando no estoy leyendo, me gusta pasar tiempo al aire libre. Me encanta hacer senderismo en las montañas y explorar los senderos naturales. Hay algo en estar rodeado de naturaleza que me ayuda a despejar la mente y encontrar la paz. También disfruto acampar bajo las estrellas y escuchar los sonidos del bosque por la noche. Es una excelente manera de desconectar del ajetreo y el bullicio de la vida cotidiana.\n",
      "Average generation time per character: 0.08663049690863665 seconds\n",
      "Average generation time per token: 0.49782032026371487 seconds\n",
      "Average audio duration per character: 0.11328186274509804 seconds\n",
      "Average audio duration per token: 0.6509718309859155 seconds\n",
      "\n",
      "Text: Une autre passion que j'ai est la technologie. Je suis toujours excité d'apprendre les dernières avancées en intelligence artificielle, en robotique et en exploration spatiale. Je crois que la technologie a le potentiel de résoudre de nombreux problèmes mondiaux et d'améliorer notre qualité de vie. Par exemple, l'IA peut nous aider à prendre de meilleures décisions en analysant de grandes quantités de données, tandis que les robots peuvent aider dans des tâches dangereuses ou répétitives.\n",
      "Average generation time per character: 0.06253827250733668 seconds\n",
      "Average generation time per token: 0.4085833803812663 seconds\n",
      "Average audio duration per character: 0.08423061224489796 seconds\n",
      "Average audio duration per token: 0.5503066666666667 seconds\n",
      "\n",
      "Text: Quando si parla di esplorazione spaziale, sono un grande fan della NASA e di SpaceX. L'idea che gli esseri umani possano viaggiare su Marte e oltre è incredibilmente emozionante. Seguo da vicino le ultime missioni e sviluppi, e spero di vedere una missione umana su Marte nella mia vita. Il pensiero di esplorare nuovi mondi e possibilmente trovare vita extraterrestre ha sempre alimentato la mia immaginazione.\n",
      "Average generation time per character: 0.09013448392643648 seconds\n",
      "Average generation time per token: 0.5571949915452437 seconds\n",
      "Average audio duration per character: 0.12310049019607842 seconds\n",
      "Average audio duration per token: 0.7609848484848484 seconds\n",
      "\n",
      "Text: 音楽は私の人生のもう一つの重要な部分です。私はクラシックからロック、電子音楽まで、さまざまなジャンルの音楽を聴くのが好きです。 音楽には感情を呼び起こし、思い出を作る力があると思います。 時々、ギターを弾いて自分の曲を作るのも好きです。 それは自分を表現し、長い一日の後にリラックスする素晴らしい方法です。\n",
      "Average generation time per character: 0.030010968251945148 seconds\n",
      "Average generation time per token: 1.1479195356369019 seconds\n",
      "Average audio duration per character: 0.03991503267973856 seconds\n",
      "Average audio duration per token: 1.52675 seconds\n",
      "\n",
      "Text: Por ejemplo, las nueces de Brasil se llaman 'castañas de Brasil' en español, y son deliciosas. Me gusta comerlas como un snack saludable. Además, creo en la importancia de devolver algo a la comunidad. Trabajo como voluntario en refugios locales y participo en varios eventos benéficos. Ayudar a los demás y tener un impacto positivo en el mundo es algo que me esfuerzo por lograr. Creo que incluso pequeños actos de bondad pueden hacer una gran diferencia en la vida de alguien.\n",
      "Average generation time per character: 0.06919955704282607 seconds\n",
      "Average generation time per token: 0.40000719558901904 seconds\n",
      "Average audio duration per character: 0.09292616033755273 seconds\n",
      "Average audio duration per token: 0.5371585365853658 seconds\n",
      "\n",
      "Overall Averages:\n",
      "Overall average generation time per character: 0.08058659382140704 seconds\n",
      "Overall average generation time per token: 0.5169773898275085 seconds\n",
      "Overall average audio duration per character: 0.1064346054068992 seconds\n",
      "Overall average audio duration per token: 0.6827969998647953 seconds\n",
      "\n",
      "Comparison of Texts:\n",
      "1. Text: 音楽は私の人生のもう一つの重要な部分です。私はクラシックからロック、電子音楽まで、さまざまなジャンル...\n",
      "   Average generation time per character: 0.030010968251945148 seconds\n",
      "   Average generation time per token: 1.1479195356369019 seconds\n",
      "   Average audio duration per character: 0.03991503267973856 seconds\n",
      "   Average audio duration per token: 1.52675 seconds\n",
      "\n",
      "2. Text: Por ejemplo, las nueces de Brasil se llaman 'casta...\n",
      "   Average generation time per character: 0.05492892417501896 seconds\n",
      "   Average generation time per token: 0.3227074295282364 seconds\n",
      "   Average audio duration per character: 0.07681914893617021 seconds\n",
      "   Average audio duration per token: 0.4513125 seconds\n",
      "\n",
      "3. Text: Hallo, mein Name ist Suno. Ich bin ein großer Fan ...\n",
      "   Average generation time per character: 0.0596452772479685 seconds\n",
      "   Average generation time per token: 0.4035375788807869 seconds\n",
      "   Average audio duration per character: 0.0821270207852194 seconds\n",
      "   Average audio duration per token: 0.555640625 seconds\n",
      "\n",
      "4. Text: I like reading....\n",
      "   Average generation time per character: 0.061373297373453775 seconds\n",
      "   Average generation time per token: 0.30686648686726886 seconds\n",
      "   Average audio duration per character: 0.07353333333333333 seconds\n",
      "   Average audio duration per token: 0.36766666666666664 seconds\n",
      "\n",
      "5. Text: Une autre passion que j'ai est la technologie. Je ...\n",
      "   Average generation time per character: 0.06253827250733668 seconds\n",
      "   Average generation time per token: 0.4085833803812663 seconds\n",
      "   Average audio duration per character: 0.08423061224489796 seconds\n",
      "   Average audio duration per token: 0.5503066666666667 seconds\n",
      "\n",
      "6. Text: Une autre passion que j'ai est la technologie....\n",
      "   Average generation time per character: 0.06662282218103824 seconds\n",
      "   Average generation time per token: 0.38308122754096985 seconds\n",
      "   Average audio duration per character: 0.08884782608695652 seconds\n",
      "   Average audio duration per token: 0.510875 seconds\n",
      "\n",
      "7. Text: Quando si parla di esplorazione spaziale, sono un ...\n",
      "   Average generation time per character: 0.06681059939520699 seconds\n",
      "   Average generation time per token: 0.37413935661315917 seconds\n",
      "   Average audio duration per character: 0.09260714285714286 seconds\n",
      "   Average audio duration per token: 0.5186 seconds\n",
      "\n",
      "8. Text: Por ejemplo, las nueces de Brasil se llaman 'casta...\n",
      "   Average generation time per character: 0.06919955704282607 seconds\n",
      "   Average generation time per token: 0.40000719558901904 seconds\n",
      "   Average audio duration per character: 0.09292616033755273 seconds\n",
      "   Average audio duration per token: 0.5371585365853658 seconds\n",
      "\n",
      "9. Text: Cuando no estoy leyendo, me gusta pasar tiempo al ...\n",
      "   Average generation time per character: 0.07850398391973777 seconds\n",
      "   Average generation time per token: 0.43534027446400037 seconds\n",
      "   Average audio duration per character: 0.10924590163934425 seconds\n",
      "   Average audio duration per token: 0.6058181818181818 seconds\n",
      "\n",
      "10. Text: J'aime la technologie....\n",
      "   Average generation time per character: 0.07908084175803444 seconds\n",
      "   Average generation time per token: 0.5799261728922526 seconds\n",
      "   Average audio duration per character: 0.1029090909090909 seconds\n",
      "   Average audio duration per token: 0.7546666666666666 seconds\n",
      "\n",
      "11. Text: Cuando no estoy leyendo, me gusta pasar tiempo al ...\n",
      "   Average generation time per character: 0.08663049690863665 seconds\n",
      "   Average generation time per token: 0.49782032026371487 seconds\n",
      "   Average audio duration per character: 0.11328186274509804 seconds\n",
      "   Average audio duration per token: 0.6509718309859155 seconds\n",
      "\n",
      "12. Text: Quando si parla di esplorazione spaziale, sono un ...\n",
      "   Average generation time per character: 0.09013448392643648 seconds\n",
      "   Average generation time per token: 0.5571949915452437 seconds\n",
      "   Average audio duration per character: 0.12310049019607842 seconds\n",
      "   Average audio duration per token: 0.7609848484848484 seconds\n",
      "\n",
      "13. Text: 音楽は私の人生のもう一つの重要な部分です。...\n",
      "   Average generation time per character: 0.1161147639864967 seconds\n",
      "   Average generation time per token: 2.4384100437164307 seconds\n",
      "   Average audio duration per character: 0.14871428571428572 seconds\n",
      "   Average audio duration per token: 3.123 seconds\n",
      "\n",
      "14. Text: Ich mag Pizza....\n",
      "   Average generation time per character: 0.13582447596958705 seconds\n",
      "   Average generation time per token: 0.6338475545247396 seconds\n",
      "   Average audio duration per character: 0.1617142857142857 seconds\n",
      "   Average audio duration per token: 0.7546666666666666 seconds\n",
      "\n",
      "15. Text: Hallo, mein Name ist Suno. Und, äh — und ich mag P...\n",
      "   Average generation time per character: 0.15919128612235742 seconds\n",
      "   Average generation time per token: 0.7163607875506083 seconds\n",
      "   Average audio duration per character: 0.2177962962962963 seconds\n",
      "   Average audio duration per token: 0.9800833333333333 seconds\n",
      "\n",
      "16. Text: 音楽が好きです。...\n",
      "   Average generation time per character: 0.1688278317451477 seconds\n",
      "   Average generation time per token: 1.3506226539611816 seconds\n",
      "   Average audio duration per character: 0.1945 seconds\n",
      "   Average audio duration per token: 1.556 seconds\n",
      "\n",
      "17. Text: Mi piace la musica....\n",
      "   Average generation time per character: 0.18249358628925524 seconds\n",
      "   Average generation time per token: 0.8668445348739624 seconds\n",
      "   Average audio duration per character: 0.242 seconds\n",
      "   Average audio duration per token: 1.1495 seconds\n",
      "\n",
      "18. Text: こんにちは。...\n",
      "   Average generation time per character: 0.1842040220896403 seconds\n",
      "   Average generation time per token: 1.1052241325378418 seconds\n",
      "   Average audio duration per character: 0.18383333333333332 seconds\n",
      "   Average audio duration per token: 1.103 seconds\n",
      "\n",
      "19. Text: Hello....\n",
      "   Average generation time per character: 0.18752539157867432 seconds\n",
      "   Average generation time per token: 1.125152349472046 seconds\n",
      "   Average audio duration per character: 0.2011666666666667 seconds\n",
      "   Average audio duration per token: 1.207 seconds\n",
      "\n",
      "20. Text: Me gusta leer....\n",
      "   Average generation time per character: 0.19866129330226354 seconds\n",
      "   Average generation time per token: 0.9270860354105631 seconds\n",
      "   Average audio duration per character: 0.25207142857142856 seconds\n",
      "   Average audio duration per token: 1.1763333333333332 seconds\n",
      "\n",
      "21. Text: Ciao....\n",
      "   Average generation time per character: 0.28762102127075195 seconds\n",
      "   Average generation time per token: 1.4381051063537598 seconds\n",
      "   Average audio duration per character: 0.33199999999999996 seconds\n",
      "   Average audio duration per token: 1.66 seconds\n",
      "\n",
      "22. Text: Bonjour....\n",
      "   Average generation time per character: 0.3820926547050476 seconds\n",
      "   Average generation time per token: 3.056741237640381 seconds\n",
      "   Average audio duration per character: 0.49775 seconds\n",
      "   Average audio duration per token: 3.982 seconds\n",
      "\n",
      "23. Text: Hola....\n",
      "   Average generation time per character: 0.4366607189178467 seconds\n",
      "   Average generation time per token: 2.1833035945892334 seconds\n",
      "   Average audio duration per character: 0.5456000000000001 seconds\n",
      "   Average audio duration per token: 2.728 seconds\n",
      "\n",
      "24. Text: Hallo....\n",
      "   Average generation time per character: 2.2236289580663047 seconds\n",
      "   Average generation time per token: 13.341773748397827 seconds\n",
      "   Average audio duration per character: 2.291 seconds\n",
      "   Average audio duration per token: 13.746 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from pydub import AudioSegment\n",
    "import soundfile as sf\n",
    "from TTS.tts.configs.xtts_config import XttsConfig\n",
    "from TTS.tts.models.xtts import Xtts\n",
    "\n",
    "# Download the necessary NLTK data files\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Define a list of test texts in different languages and with mixed grammar\n",
    "test_texts = [\n",
    "    # Short and simple sentences\n",
    "    \"Hallo.\",\n",
    "    \"Ich mag Pizza.\",\n",
    "    \"Hola.\",\n",
    "    \"Me gusta leer.\",\n",
    "    \"Bonjour.\",\n",
    "    \"J'aime la technologie.\",\n",
    "    \"Ciao.\",\n",
    "    \"Mi piace la musica.\",\n",
    "    \"こんにちは。\",\n",
    "    \"音楽が好きです。\",\n",
    "    \"Hello.\",\n",
    "    \"I like reading.\",\n",
    "\n",
    "    # Longer and more complex sentences\n",
    "    \"Hallo, mein Name ist Suno. Und, äh — und ich mag Pizza.\",\n",
    "    \"Cuando no estoy leyendo, me gusta pasar tiempo al aire libre.\",\n",
    "    \"Une autre passion que j'ai est la technologie.\",\n",
    "    \"Quando si parla di esplorazione spaziale, sono un grande fan della NASA e di SpaceX.\",\n",
    "    \"音楽は私の人生のもう一つの重要な部分です。\",\n",
    "    \"Por ejemplo, las nueces de Brasil se llaman 'castañas de Brasil' en español, y son deliciosas.\",\n",
    "\n",
    "    # Very long and complex sentences\n",
    "    \"Hallo, mein Name ist Suno. Ich bin ein großer Fan von Pizza, besonders wenn sie mit frischen Zutaten und viel Käse zubereitet wird. \"\n",
    "    \"In meiner Freizeit lese ich gerne Bücher, besonders Science-Fiction- und Fantasy-Romane. \"\n",
    "    \"Einer meiner Lieblingsautoren ist Isaac Asimov, und ich liebe besonders seine Foundation-Serie. \"\n",
    "    \"Die Art und Weise, wie er komplexe Erzählungen webt und die Zukunft der Menschheit erforscht, ist wirklich faszinierend.\",\n",
    "\n",
    "    \"Cuando no estoy leyendo, me gusta pasar tiempo al aire libre. Me encanta hacer senderismo en las montañas y explorar los senderos naturales. \"\n",
    "    \"Hay algo en estar rodeado de naturaleza que me ayuda a despejar la mente y encontrar la paz. \"\n",
    "    \"También disfruto acampar bajo las estrellas y escuchar los sonidos del bosque por la noche. \"\n",
    "    \"Es una excelente manera de desconectar del ajetreo y el bullicio de la vida cotidiana.\",\n",
    "\n",
    "    \"Une autre passion que j'ai est la technologie. Je suis toujours excité d'apprendre les dernières avancées en intelligence artificielle, en robotique et en exploration spatiale. \"\n",
    "    \"Je crois que la technologie a le potentiel de résoudre de nombreux problèmes mondiaux et d'améliorer notre qualité de vie. \"\n",
    "    \"Par exemple, l'IA peut nous aider à prendre de meilleures décisions en analysant de grandes quantités de données, tandis que les robots peuvent aider dans des tâches dangereuses ou répétitives.\",\n",
    "\n",
    "    \"Quando si parla di esplorazione spaziale, sono un grande fan della NASA e di SpaceX. L'idea che gli esseri umani possano viaggiare su Marte e oltre è incredibilmente emozionante. \"\n",
    "    \"Seguo da vicino le ultime missioni e sviluppi, e spero di vedere una missione umana su Marte nella mia vita. \"\n",
    "    \"Il pensiero di esplorare nuovi mondi e possibilmente trovare vita extraterrestre ha sempre alimentato la mia immaginazione.\",\n",
    "\n",
    "    \"音楽は私の人生のもう一つの重要な部分です。私はクラシックからロック、電子音楽まで、さまざまなジャンルの音楽を聴くのが好きです。 \"\n",
    "    \"音楽には感情を呼び起こし、思い出を作る力があると思います。 \"\n",
    "    \"時々、ギターを弾いて自分の曲を作るのも好きです。 \"\n",
    "    \"それは自分を表現し、長い一日の後にリラックスする素晴らしい方法です。\",\n",
    "\n",
    "    \"Por ejemplo, las nueces de Brasil se llaman 'castañas de Brasil' en español, y son deliciosas. \"\n",
    "    \"Me gusta comerlas como un snack saludable. \"\n",
    "    \"Además, creo en la importancia de devolver algo a la comunidad. \"\n",
    "    \"Trabajo como voluntario en refugios locales y participo en varios eventos benéficos. \"\n",
    "    \"Ayudar a los demás y tener un impacto positivo en el mundo es algo que me esfuerzo por lograr. \"\n",
    "    \"Creo que incluso pequeños actos de bondad pueden hacer una gran diferencia en la vida de alguien.\"\n",
    "]\n",
    "\n",
    "# Load the TTS model\n",
    "config = XttsConfig()\n",
    "config.load_json(\"XTTS-v2\\\\config.json\")\n",
    "model = Xtts.init_from_config(config)\n",
    "model.load_checkpoint(config, checkpoint_dir=\"XTTS-v2\", eval=True)\n",
    "model.cuda()\n",
    "\n",
    "# Function to measure generation time and audio duration for each sentence and character\n",
    "def measure_generation_time_and_audio_duration(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    total_time = 0\n",
    "    total_audio_duration = 0\n",
    "    total_chars = 0\n",
    "    total_tokens = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        start_time = time.time()\n",
    "        outputs = model.synthesize(\n",
    "            text=sentence,\n",
    "            config=config,\n",
    "            speaker_wav=\"XTTS-v2\\\\samples\\\\morgan_freeman.wav\",\n",
    "            gpt_cond_len=30,\n",
    "            language=\"de\",\n",
    "            speed=1.0\n",
    "        )\n",
    "        end_time = time.time()\n",
    "        generation_time = end_time - start_time\n",
    "\n",
    "        # Save the generated audio to a temporary file and measure its duration\n",
    "        temp_audio_path = \"temp_audio.wav\"\n",
    "        sf.write(temp_audio_path, outputs['wav'], config.audio.sample_rate)\n",
    "        audio = AudioSegment.from_wav(temp_audio_path)\n",
    "        audio_duration = len(audio) / 1000.0  # Convert from milliseconds to seconds\n",
    "\n",
    "        total_time += generation_time\n",
    "        total_audio_duration += audio_duration\n",
    "        total_chars += len(sentence)\n",
    "        total_tokens += len(sentence.split())\n",
    "\n",
    "    avg_time_per_char = total_time / total_chars if total_chars > 0 else 0\n",
    "    avg_time_per_token = total_time / total_tokens if total_tokens > 0 else 0\n",
    "    avg_audio_duration_per_char = total_audio_duration / total_chars if total_chars > 0 else 0\n",
    "    avg_audio_duration_per_token = total_audio_duration / total_tokens if total_tokens > 0 else 0\n",
    "\n",
    "    return avg_time_per_char, avg_time_per_token, avg_audio_duration_per_char, avg_audio_duration_per_token\n",
    "\n",
    "# Run the test for each text and calculate the rates\n",
    "results = []\n",
    "\n",
    "for text in test_texts:\n",
    "    avg_time_per_char, avg_time_per_token, avg_audio_duration_per_char, avg_audio_duration_per_token = measure_generation_time_and_audio_duration(text)\n",
    "    results.append({\n",
    "        \"text\": text,\n",
    "        \"avg_time_per_char\": avg_time_per_char,\n",
    "        \"avg_time_per_token\": avg_time_per_token,\n",
    "        \"avg_audio_duration_per_char\": avg_audio_duration_per_char,\n",
    "        \"avg_audio_duration_per_token\": avg_audio_duration_per_token\n",
    "    })\n",
    "\n",
    "# Print the results and calculate overall averages\n",
    "total_chars = 0\n",
    "total_tokens = 0\n",
    "total_time = 0\n",
    "total_audio_duration = 0\n",
    "\n",
    "for result in results:\n",
    "    print(f\"Text: {result['text']}\")\n",
    "    print(f\"Average generation time per character: {result['avg_time_per_char']} seconds\")\n",
    "    print(f\"Average generation time per token: {result['avg_time_per_token']} seconds\")\n",
    "    print(f\"Average audio duration per character: {result['avg_audio_duration_per_char']} seconds\")\n",
    "    print(f\"Average audio duration per token: {result['avg_audio_duration_per_token']} seconds\")\n",
    "    print()\n",
    "    \n",
    "    total_chars += len(result['text'])\n",
    "    total_tokens += len(result['text'].split())\n",
    "    total_time += result['avg_time_per_char'] * len(result['text'])\n",
    "    total_audio_duration += result['avg_audio_duration_per_char'] * len(result['text'])\n",
    "\n",
    "# Calculate overall averages\n",
    "overall_avg_time_per_char = total_time / total_chars if total_chars > 0 else 0\n",
    "overall_avg_time_per_token = total_time / total_tokens if total_tokens > 0 else 0\n",
    "overall_avg_audio_duration_per_char = total_audio_duration / total_chars if total_chars > 0 else 0\n",
    "overall_avg_audio_duration_per_token = total_audio_duration / total_tokens if total_tokens > 0 else 0\n",
    "\n",
    "print(\"Overall Averages:\")\n",
    "print(f\"Overall average generation time per character: {overall_avg_time_per_char} seconds\")\n",
    "print(f\"Overall average generation time per token: {overall_avg_time_per_token} seconds\")\n",
    "print(f\"Overall average audio duration per character: {overall_avg_audio_duration_per_char} seconds\")\n",
    "print(f\"Overall average audio duration per token: {overall_avg_audio_duration_per_token} seconds\")\n",
    "\n",
    "# Compare texts to each other\n",
    "print(\"\\nComparison of Texts:\")\n",
    "sorted_results = sorted(results, key=lambda x: x['avg_time_per_char'])\n",
    "for i, result in enumerate(sorted_results):\n",
    "    print(f\"{i+1}. Text: {result['text'][:50]}...\")  # Print the first 50 characters for brevity\n",
    "    print(f\"   Average generation time per character: {result['avg_time_per_char']} seconds\")\n",
    "    print(f\"   Average generation time per token: {result['avg_time_per_token']} seconds\")\n",
    "    print(f\"   Average audio duration per character: {result['avg_audio_duration_per_char']} seconds\")\n",
    "    print(f\"   Average audio duration per token: {result['avg_audio_duration_per_token']} seconds\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pydub import AudioSegment\n",
    "import soundfile as sf\n",
    "import threading\n",
    "from pydub.playback import play\n",
    "import os\n",
    "import gc\n",
    "from TTS.tts.configs.xtts_config import XttsConfig\n",
    "from TTS.tts.models.xtts import Xtts\n",
    "import torch\n",
    "\n",
    "class TTSStreamer:\n",
    "    def __init__(self, model_path, config_path, vocab_path, speaker_wav=\"XTTS-v2\\\\samples\\\\morgan_freeman.wav\"):\n",
    "        self.model_path = model_path\n",
    "        self.config_path = config_path\n",
    "        self.vocab_path = vocab_path\n",
    "        self.speaker_wav = speaker_wav\n",
    "        self.model = self.load_model()\n",
    "\n",
    "    def load_model(self):\n",
    "        config = XttsConfig()\n",
    "        config.load_json(self.config_path)\n",
    "        model = Xtts.init_from_config(config)\n",
    "        model.load_checkpoint(config, checkpoint_dir=self.model_path, eval=True, vocab_path=self.vocab_path)\n",
    "        model.cuda()\n",
    "        return model\n",
    "\n",
    "\n",
    "    def unload_model(self):\n",
    "        del self.model\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "        print(\"Model unloaded and GPU memory cleared successfully.\")\n",
    "\n",
    "    def estimate_times(self, text_chunk, avg_gen_time_per_char, avg_audio_time_per_char):\n",
    "        gen_time = len(text_chunk) * avg_gen_time_per_char\n",
    "        audio_duration = len(text_chunk) * avg_audio_time_per_char\n",
    "        return gen_time, audio_duration\n",
    "\n",
    "    def split_text_into_lines(self, text):\n",
    "        lines = text.strip().split('\\n')\n",
    "        return lines\n",
    "\n",
    "    def generate_audio_chunk(self, chunk, chunk_index, audio_buffer, playback_event, avg_gen_time_per_char, avg_audio_time_per_char, total_gen_time, language, speed):\n",
    "        est_gen_time, est_audio_duration = self.estimate_times(chunk, avg_gen_time_per_char, avg_audio_time_per_char)\n",
    "        print(f\"Chunk {chunk_index + 1} estimated generation time: {est_gen_time:.2f} seconds, estimated audio duration: {est_audio_duration:.2f} seconds\")\n",
    "\n",
    "        print(f\"Generating audio for chunk {chunk_index + 1}...\")\n",
    "        start_gen_time = time.time()\n",
    "        outputs = self.model.synthesize(\n",
    "            text=chunk,\n",
    "            config=self.model.config,\n",
    "            speaker_wav=self.speaker_wav,\n",
    "            gpt_cond_len=30,\n",
    "            language=language,\n",
    "            speed=speed\n",
    "        )\n",
    "        end_gen_time = time.time()\n",
    "        generation_time = end_gen_time - start_gen_time\n",
    "        total_gen_time[0] += generation_time\n",
    "        print(f\"Chunk {chunk_index + 1} generated in {generation_time:.2f} seconds (estimated: {est_gen_time:.2f} seconds)\")\n",
    "\n",
    "        wav_data = outputs['wav']\n",
    "        temp_output_file = f'temp_output_{chunk_index}.wav'\n",
    "        sf.write(temp_output_file, wav_data, 22050)\n",
    "        line_audio = AudioSegment.from_wav(temp_output_file)\n",
    "\n",
    "        actual_audio_duration = len(line_audio) / 1000.0\n",
    "        print(f\"Chunk {chunk_index + 1} actual audio duration: {actual_audio_duration:.2f} seconds (estimated: {est_audio_duration:.2f} seconds)\")\n",
    "\n",
    "        audio_buffer[chunk_index] = line_audio\n",
    "        print(f\"Chunk {chunk_index + 1} audio saved and buffered\")\n",
    "\n",
    "        playback_event.set()\n",
    "\n",
    "    def stream_audio_with_buffering(self, text, language=\"en\", speed=1.2, speaker_path=None, fireup_delay=1.0, avg_gen_time_per_char=0.08058659382140704, avg_audio_time_per_char=0.1064346054068992):\n",
    "        if speaker_path:\n",
    "            self.speaker_wav = speaker_path\n",
    "\n",
    "        print(\"Starting the audio streaming process...\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        text_chunks = self.split_text_into_lines(text)\n",
    "        audio_buffer = [None] * len(text_chunks)\n",
    "        playback_events = [threading.Event() for _ in text_chunks]\n",
    "        total_gen_time = [0]\n",
    "\n",
    "        def start_playback_after_delay():\n",
    "            print(f\"Waiting {fireup_delay:.2f} seconds before starting playback...\")\n",
    "            time.sleep(fireup_delay)\n",
    "            print(\"Fireup delay is over, starting playback...\")\n",
    "            for chunk_index in range(len(text_chunks)):\n",
    "                playback_events[chunk_index].wait()\n",
    "                if audio_buffer[chunk_index] is not None:\n",
    "                    self.play_audio_segment(audio_buffer[chunk_index])\n",
    "\n",
    "        playback_thread = threading.Thread(target=start_playback_after_delay)\n",
    "        playback_thread.start()\n",
    "\n",
    "        for chunk_index, chunk in enumerate(text_chunks):\n",
    "            print(f\"Processing chunk {chunk_index + 1}/{len(text_chunks)}: '{chunk}'\")\n",
    "            self.generate_audio_chunk(chunk, chunk_index, audio_buffer, playback_events[chunk_index], avg_gen_time_per_char, avg_audio_time_per_char, total_gen_time, language, speed)\n",
    "\n",
    "        playback_thread.join()\n",
    "        print(\"Audio streaming process completed.\")\n",
    "        print(f\"Total generation time: {total_gen_time[0]:.2f} seconds\")\n",
    "\n",
    "    def play_audio_segment(self, audio_segment):\n",
    "        play(audio_segment)\n",
    "\n",
    "\n",
    "# Initialize the TTSStreamer\n",
    "tts_streamer = TTSStreamer(model_path=\"XTTS-v2\", config_path=\"XTTS-v2\\\\config.json\", vocab_path=\"XTTS-v2\\\\vocab.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the audio streaming process...\n",
      "Waiting 5.00 seconds before starting playback...\n",
      "Processing chunk 1/8: 'A series of extracts from Middle English texts of different dates and types is presented, with commentary on all of the loanwords '\n",
      "Chunk 1 estimated generation time: 10.48 seconds, estimated audio duration: 13.84 seconds\n",
      "Generating audio for chunk 1...\n",
      "Fireup delay is over, starting playback...\n",
      "Chunk 1 generated in 7.79 seconds (estimated: 10.48 seconds)\n",
      "Chunk 1 actual audio duration: 7.02 seconds (estimated: 13.84 seconds)\n",
      "Chunk 1 audio saved and buffered\n",
      "Processing chunk 2/8: '(from French, Latin, or early Scandinavian) found in each passage. '\n",
      "Chunk 2 estimated generation time: 5.40 seconds, estimated audio duration: 7.13 seconds\n",
      "Generating audio for chunk 2...\n",
      "Chunk 2 generated in 3.68 seconds (estimated: 5.40 seconds)\n",
      "Chunk 2 actual audio duration: 3.79 seconds (estimated: 7.13 seconds)\n",
      "Chunk 2 audio saved and buffered\n",
      "Processing chunk 3/8: 'The extracts are drawn from the Final Continuation of the Peterborough Chronicle; '\n",
      "Chunk 3 estimated generation time: 6.61 seconds, estimated audio duration: 8.73 seconds\n",
      "Generating audio for chunk 3...\n",
      "Chunk 3 generated in 3.62 seconds (estimated: 6.61 seconds)\n",
      "Chunk 3 actual audio duration: 3.74 seconds (estimated: 8.73 seconds)\n",
      "Chunk 3 audio saved and buffered\n",
      "Processing chunk 4/8: 'the Ormulum; the Ancrene Wisse; John Trevisa’s translation of Ranulf Higden’s Polychronicon; and Caxton’s Prologue to The Boke of Eneydos. '\n",
      "Chunk 4 estimated generation time: 11.20 seconds, estimated audio duration: 14.79 seconds\n",
      "Generating audio for chunk 4...\n",
      "Chunk 4 generated in 7.60 seconds (estimated: 11.20 seconds)\n",
      "Chunk 4 actual audio duration: 8.23 seconds (estimated: 14.79 seconds)\n",
      "Chunk 4 audio saved and buffered\n",
      "Processing chunk 5/8: 'This is complemented by a more discursive consideration of some passages from multilingual texts and texts not in English, '\n",
      "Chunk 5 estimated generation time: 9.91 seconds, estimated audio duration: 13.09 seconds\n",
      "Generating audio for chunk 5...\n",
      "Chunk 5 generated in 6.37 seconds (estimated: 9.91 seconds)\n",
      "Chunk 5 actual audio duration: 6.77 seconds (estimated: 13.09 seconds)\n",
      "Chunk 5 audio saved and buffered\n",
      "Processing chunk 6/8: 'drawing attention to the questions they raise about some of the likely mechanisms by which words entered English, '\n",
      "Chunk 6 estimated generation time: 9.19 seconds, estimated audio duration: 12.13 seconds\n",
      "Generating audio for chunk 6...\n",
      "Chunk 6 generated in 4.81 seconds (estimated: 9.19 seconds)\n",
      "Chunk 6 actual audio duration: 5.05 seconds (estimated: 12.13 seconds)\n",
      "Chunk 6 audio saved and buffered\n",
      "Processing chunk 7/8: 'as well about how far individuals distinguished between the vocabularies of each language in a situation of functional language switching. '\n",
      "Chunk 7 estimated generation time: 11.20 seconds, estimated audio duration: 14.79 seconds\n",
      "Generating audio for chunk 7...\n",
      "Chunk 7 generated in 6.44 seconds (estimated: 11.20 seconds)\n",
      "Chunk 7 actual audio duration: 6.87 seconds (estimated: 14.79 seconds)\n",
      "Chunk 7 audio saved and buffered\n",
      "Processing chunk 8/8: 'The chapter is followed by a brief summary of the main conclusions from chapters 11, 12, and 13.'\n",
      "Chunk 8 estimated generation time: 7.74 seconds, estimated audio duration: 10.22 seconds\n",
      "Generating audio for chunk 8...\n",
      "Chunk 8 generated in 4.97 seconds (estimated: 7.74 seconds)\n",
      "Chunk 8 actual audio duration: 5.20 seconds (estimated: 10.22 seconds)\n",
      "Chunk 8 audio saved and buffered\n",
      "Audio streaming process completed.\n",
      "Total generation time: 45.27 seconds\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "text = \"\"\"\n",
    "A series of extracts from Middle English texts of different dates and types is presented, with commentary on all of the loanwords \n",
    "(from French, Latin, or early Scandinavian) found in each passage. \n",
    "The extracts are drawn from the Final Continuation of the Peterborough Chronicle; \n",
    "the Ormulum; the Ancrene Wisse; John Trevisa’s translation of Ranulf Higden’s Polychronicon; and Caxton’s Prologue to The Boke of Eneydos. \n",
    "This is complemented by a more discursive consideration of some passages from multilingual texts and texts not in English, \n",
    "drawing attention to the questions they raise about some of the likely mechanisms by which words entered English, \n",
    "as well about how far individuals distinguished between the vocabularies of each language in a situation of functional language switching. \n",
    "The chapter is followed by a brief summary of the main conclusions from chapters 11, 12, and 13.\n",
    "\"\"\"\n",
    "# Stream audio with buffering\n",
    "tts_streamer.stream_audio_with_buffering(text, language=\"en\", speed=1.3, speaker_path=\"XTTS-v2\\samples\\Eren_test.wav\", fireup_delay=5.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the audio streaming process...\n",
      "Waiting 5.00 seconds before starting playback...\n",
      "Processing chunk 1/13: '1. Was genau macht MultilingualPress?'\n",
      "Chunk 1 estimated generation time: 2.98 seconds, estimated audio duration: 3.94 seconds\n",
      "Generating audio for chunk 1...\n",
      "Chunk 1 generated in 3.34 seconds (estimated: 2.98 seconds)\n",
      "Chunk 1 actual audio duration: 3.33 seconds (estimated: 3.94 seconds)\n",
      "Chunk 1 audio saved and buffered\n",
      "Processing chunk 2/13: 'MultilingualPress stellt Sprachbeziehungen zwischen verschiedenen Sites einer WordPress Multisite her. '\n",
      "Chunk 2 estimated generation time: 8.30 seconds, estimated audio duration: 10.96 seconds\n",
      "Generating audio for chunk 2...\n",
      "Fireup delay is over, starting playback...\n",
      "Chunk 2 generated in 6.82 seconds (estimated: 8.30 seconds)\n",
      "Chunk 2 actual audio duration: 7.22 seconds (estimated: 10.96 seconds)\n",
      "Chunk 2 audio saved and buffered\n",
      "Processing chunk 3/13: 'Somit ermöglicht es dir, deinen Websitebesuchern Inhalte in der für sie passenden Sprachversion anzuzeigen. '\n",
      "Chunk 3 estimated generation time: 8.70 seconds, estimated audio duration: 11.49 seconds\n",
      "Generating audio for chunk 3...\n",
      "Chunk 3 generated in 7.37 seconds (estimated: 8.70 seconds)\n",
      "Chunk 3 actual audio duration: 7.83 seconds (estimated: 11.49 seconds)\n",
      "Chunk 3 audio saved and buffered\n",
      "Processing chunk 4/13: 'Dabei legst du jede Sprachversion deiner Website als Site in einer WordPress Multisite an und verknüpfst die übersetzten Inhalte mithilfe von MultilingualPress. '\n",
      "Chunk 4 estimated generation time: 12.97 seconds, estimated audio duration: 17.14 seconds\n",
      "Generating audio for chunk 4...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 16\u001b[0m\n\u001b[0;32m      1\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124m1. Was genau macht MultilingualPress?\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124mMultilingualPress stellt Sprachbeziehungen zwischen verschiedenen Sites einer WordPress Multisite her. \u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;124mSollte das Video nicht alle deine Fragen beantwortet haben, dann lies bitte hier weiter oder melde dich bei uns.\u001b[39m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m---> 16\u001b[0m \u001b[43mtts_streamer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream_audio_with_buffering\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mde\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspeed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspeaker_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mXTTS-v2\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43msamples\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mEren_test.wav\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfireup_delay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5.0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 104\u001b[0m, in \u001b[0;36mTTSStreamer.stream_audio_with_buffering\u001b[1;34m(self, text, language, speed, speaker_path, fireup_delay, avg_gen_time_per_char, avg_audio_time_per_char)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk_index, chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(text_chunks):\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing chunk \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunk_index\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text_chunks)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 104\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_audio_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplayback_events\u001b[49m\u001b[43m[\u001b[49m\u001b[43mchunk_index\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mavg_gen_time_per_char\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mavg_audio_time_per_char\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_gen_time\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspeed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    106\u001b[0m playback_thread\u001b[38;5;241m.\u001b[39mjoin()\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAudio streaming process completed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 52\u001b[0m, in \u001b[0;36mTTSStreamer.generate_audio_chunk\u001b[1;34m(self, chunk, chunk_index, audio_buffer, playback_event, avg_gen_time_per_char, avg_audio_time_per_char, total_gen_time, language, speed)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerating audio for chunk \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunk_index\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     51\u001b[0m start_gen_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 52\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msynthesize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspeaker_wav\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspeaker_wav\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgpt_cond_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlanguage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspeed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspeed\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m end_gen_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     61\u001b[0m generation_time \u001b[38;5;241m=\u001b[39m end_gen_time \u001b[38;5;241m-\u001b[39m start_gen_time\n",
      "File \u001b[1;32mc:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\TTS\\tts\\models\\xtts.py:424\u001b[0m, in \u001b[0;36mXtts.synthesize\u001b[1;34m(self, text, config, speaker_wav, language, speaker_id, **kwargs)\u001b[0m\n\u001b[0;32m    415\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minference(text, language, gpt_cond_latent, speaker_embedding, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msettings)\n\u001b[0;32m    416\u001b[0m settings\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[0;32m    417\u001b[0m     {\n\u001b[0;32m    418\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt_cond_len\u001b[39m\u001b[38;5;124m\"\u001b[39m: config\u001b[38;5;241m.\u001b[39mgpt_cond_len,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    422\u001b[0m     }\n\u001b[0;32m    423\u001b[0m )\n\u001b[1;32m--> 424\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfull_inference(text, speaker_wav, language, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msettings)\n",
      "File \u001b[1;32mc:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\TTS\\tts\\models\\xtts.py:493\u001b[0m, in \u001b[0;36mXtts.full_inference\u001b[1;34m(self, text, ref_audio_path, language, temperature, length_penalty, repetition_penalty, top_k, top_p, do_sample, gpt_cond_len, gpt_cond_chunk_len, max_ref_len, sound_norm_refs, **hf_generate_kwargs)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;124;03mThis function produces an audio clip of the given text being spoken with the given reference voice.\u001b[39;00m\n\u001b[0;32m    448\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    483\u001b[0m \u001b[38;5;124;03m    Sample rate is 24kHz.\u001b[39;00m\n\u001b[0;32m    484\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    485\u001b[0m (gpt_cond_latent, speaker_embedding) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_conditioning_latents(\n\u001b[0;32m    486\u001b[0m     audio_path\u001b[38;5;241m=\u001b[39mref_audio_path,\n\u001b[0;32m    487\u001b[0m     gpt_cond_len\u001b[38;5;241m=\u001b[39mgpt_cond_len,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m     sound_norm_refs\u001b[38;5;241m=\u001b[39msound_norm_refs,\n\u001b[0;32m    491\u001b[0m )\n\u001b[1;32m--> 493\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minference(\n\u001b[0;32m    494\u001b[0m     text,\n\u001b[0;32m    495\u001b[0m     language,\n\u001b[0;32m    496\u001b[0m     gpt_cond_latent,\n\u001b[0;32m    497\u001b[0m     speaker_embedding,\n\u001b[0;32m    498\u001b[0m     temperature\u001b[38;5;241m=\u001b[39mtemperature,\n\u001b[0;32m    499\u001b[0m     length_penalty\u001b[38;5;241m=\u001b[39mlength_penalty,\n\u001b[0;32m    500\u001b[0m     repetition_penalty\u001b[38;5;241m=\u001b[39mrepetition_penalty,\n\u001b[0;32m    501\u001b[0m     top_k\u001b[38;5;241m=\u001b[39mtop_k,\n\u001b[0;32m    502\u001b[0m     top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[0;32m    503\u001b[0m     do_sample\u001b[38;5;241m=\u001b[39mdo_sample,\n\u001b[0;32m    504\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhf_generate_kwargs,\n\u001b[0;32m    505\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\TTS\\tts\\models\\xtts.py:546\u001b[0m, in \u001b[0;36mXtts.inference\u001b[1;34m(self, text, language, gpt_cond_latent, speaker_embedding, temperature, length_penalty, repetition_penalty, top_k, top_p, do_sample, num_beams, speed, enable_text_splitting, **hf_generate_kwargs)\u001b[0m\n\u001b[0;32m    541\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[0;32m    542\u001b[0m     text_tokens\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgpt_max_text_tokens\n\u001b[0;32m    543\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m ❗ XTTS can only generate text with a maximum of 400 tokens.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    545\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 546\u001b[0m     gpt_codes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgpt\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m    547\u001b[0m         cond_latents\u001b[38;5;241m=\u001b[39mgpt_cond_latent,\n\u001b[0;32m    548\u001b[0m         text_inputs\u001b[38;5;241m=\u001b[39mtext_tokens,\n\u001b[0;32m    549\u001b[0m         input_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    550\u001b[0m         do_sample\u001b[38;5;241m=\u001b[39mdo_sample,\n\u001b[0;32m    551\u001b[0m         top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[0;32m    552\u001b[0m         top_k\u001b[38;5;241m=\u001b[39mtop_k,\n\u001b[0;32m    553\u001b[0m         temperature\u001b[38;5;241m=\u001b[39mtemperature,\n\u001b[0;32m    554\u001b[0m         num_return_sequences\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgpt_batch_size,\n\u001b[0;32m    555\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mnum_beams,\n\u001b[0;32m    556\u001b[0m         length_penalty\u001b[38;5;241m=\u001b[39mlength_penalty,\n\u001b[0;32m    557\u001b[0m         repetition_penalty\u001b[38;5;241m=\u001b[39mrepetition_penalty,\n\u001b[0;32m    558\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    559\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhf_generate_kwargs,\n\u001b[0;32m    560\u001b[0m     )\n\u001b[0;32m    561\u001b[0m     expected_output_len \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\n\u001b[0;32m    562\u001b[0m         [gpt_codes\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgpt\u001b[38;5;241m.\u001b[39mcode_stride_len], device\u001b[38;5;241m=\u001b[39mtext_tokens\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m    563\u001b[0m     )\n\u001b[0;32m    565\u001b[0m     text_len \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([text_tokens\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]], device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[1;32mc:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\TTS\\tts\\layers\\xtts\\gpt.py:589\u001b[0m, in \u001b[0;36mGPT.generate\u001b[1;34m(self, cond_latents, text_inputs, **hf_generate_kwargs)\u001b[0m\n\u001b[0;32m    582\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate\u001b[39m(\n\u001b[0;32m    583\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    584\u001b[0m     cond_latents,\n\u001b[0;32m    585\u001b[0m     text_inputs,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhf_generate_kwargs,\n\u001b[0;32m    587\u001b[0m ):\n\u001b[0;32m    588\u001b[0m     gpt_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_embeddings(cond_latents, text_inputs)\n\u001b[1;32m--> 589\u001b[0m     gen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgpt_inference\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m    590\u001b[0m         gpt_inputs,\n\u001b[0;32m    591\u001b[0m         bos_token_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_audio_token,\n\u001b[0;32m    592\u001b[0m         pad_token_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_audio_token,\n\u001b[0;32m    593\u001b[0m         eos_token_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_audio_token,\n\u001b[0;32m    594\u001b[0m         max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_gen_mel_tokens \u001b[38;5;241m+\u001b[39m gpt_inputs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],\n\u001b[0;32m    595\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhf_generate_kwargs,\n\u001b[0;32m    596\u001b[0m     )\n\u001b[0;32m    597\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_dict_in_generate\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m hf_generate_kwargs:\n\u001b[0;32m    598\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m gen\u001b[38;5;241m.\u001b[39msequences[:, gpt_inputs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] :], gen\n",
      "File \u001b[1;32mc:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\transformers\\generation\\utils.py:1622\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   1614\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   1615\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   1616\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[0;32m   1617\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   1618\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1619\u001b[0m     )\n\u001b[0;32m   1621\u001b[0m     \u001b[38;5;66;03m# 13. run sample\u001b[39;00m\n\u001b[1;32m-> 1622\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sample(\n\u001b[0;32m   1623\u001b[0m         input_ids,\n\u001b[0;32m   1624\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[0;32m   1625\u001b[0m         logits_warper\u001b[38;5;241m=\u001b[39mlogits_warper,\n\u001b[0;32m   1626\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mprepared_stopping_criteria,\n\u001b[0;32m   1627\u001b[0m         pad_token_id\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mpad_token_id,\n\u001b[0;32m   1628\u001b[0m         output_scores\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39moutput_scores,\n\u001b[0;32m   1629\u001b[0m         output_logits\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39moutput_logits,\n\u001b[0;32m   1630\u001b[0m         return_dict_in_generate\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mreturn_dict_in_generate,\n\u001b[0;32m   1631\u001b[0m         synced_gpus\u001b[38;5;241m=\u001b[39msynced_gpus,\n\u001b[0;32m   1632\u001b[0m         streamer\u001b[38;5;241m=\u001b[39mstreamer,\n\u001b[0;32m   1633\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1634\u001b[0m     )\n\u001b[0;32m   1636\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH:\n\u001b[0;32m   1637\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[0;32m   1638\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[0;32m   1639\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   1640\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1645\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[0;32m   1646\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\transformers\\generation\\utils.py:2791\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[0;32m   2788\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[0;32m   2790\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[1;32m-> 2791\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(\n\u001b[0;32m   2792\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs,\n\u001b[0;32m   2793\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   2794\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m   2795\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   2796\u001b[0m )\n\u001b[0;32m   2798\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[0;32m   2799\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\TTS\\tts\\layers\\xtts\\gpt_inference.py:95\u001b[0m, in \u001b[0;36mGPT2InferenceModel.forward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m     91\u001b[0m     emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(input_ids)\n\u001b[0;32m     92\u001b[0m     emb \u001b[38;5;241m=\u001b[39m emb \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_embedding\u001b[38;5;241m.\u001b[39mget_fixed_embedding(\n\u001b[0;32m     93\u001b[0m         attention_mask\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m (prefix_len \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), attention_mask\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m     94\u001b[0m     )\n\u001b[1;32m---> 95\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43memb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    103\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    106\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    109\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    110\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[1;32mc:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:1119\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1107\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m   1108\u001b[0m         block\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m   1109\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1116\u001b[0m         output_attentions,\n\u001b[0;32m   1117\u001b[0m     )\n\u001b[0;32m   1118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1119\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1123\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1126\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1127\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1128\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1130\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:654\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[1;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[0;32m    652\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m    653\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_2(hidden_states)\n\u001b[1;32m--> 654\u001b[0m feed_forward_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    655\u001b[0m \u001b[38;5;66;03m# residual connection\u001b[39;00m\n\u001b[0;32m    656\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m feed_forward_hidden_states\n",
      "File \u001b[1;32mc:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:575\u001b[0m, in \u001b[0;36mGPT2MLP.forward\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: Optional[Tuple[torch\u001b[38;5;241m.\u001b[39mFloatTensor]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor:\n\u001b[0;32m    574\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_fc(hidden_states)\n\u001b[1;32m--> 575\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    576\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_proj(hidden_states)\n\u001b[0;32m    577\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n",
      "File \u001b[1;32mc:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\transformers\\activations.py:56\u001b[0m, in \u001b[0;36mNewGELUActivation.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m---> 56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39mtanh(\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpi\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m (\u001b[38;5;28minput\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.044715\u001b[39m \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m3.0\u001b[39m))))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "1. Was genau macht MultilingualPress?\n",
    "MultilingualPress stellt Sprachbeziehungen zwischen verschiedenen Sites einer WordPress Multisite her. \n",
    "Somit ermöglicht es dir, deinen Websitebesuchern Inhalte in der für sie passenden Sprachversion anzuzeigen. \n",
    "Dabei legst du jede Sprachversion deiner Website als Site in einer WordPress Multisite an und verknüpfst die übersetzten Inhalte mithilfe von MultilingualPress. \n",
    "Inhalte sind beispielsweise Beiträge, Seiten, Custom Post Types oder auch sogenannte Taxonomien. Zu letzterem gehören zum Beispiel Kategorien und Schlagworte.\n",
    "\n",
    "Unser Plugin hilft dir darüberhinaus dabei, die Website-Besucher auf die passende Sprachversion weiterzuleiten. \n",
    "Es unterstützt dich automatisch auch bei der Suchmaschinenoptimierung deiner mehrsprachigen Website.\n",
    "2. Sieh dir unser MultilingualPress Video an \n",
    "Du schaust dir lieber ein Video an als einen langen Text zu lesen? Du möchtest wissen wie MultilingualPress im Backend aussieht und dich von der leichten Bedienbarkeit überzeugen, \n",
    "bevor du das Produkt kaufst? Dann sieh dir hier unser Produktvideo an.\n",
    "\n",
    "Sollte das Video nicht alle deine Fragen beantwortet haben, dann lies bitte hier weiter oder melde dich bei uns.\n",
    "\"\"\"\n",
    "tts_streamer.stream_audio_with_buffering(text, language=\"de\", speed=1.3, speaker_path=\"XTTS-v2\\samples\\Eren_test.wav\", fireup_delay=5.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "from moviepy.editor import VideoFileClip\n",
    "\n",
    "def convert_and_clip_audio(input_file, output_file, target_sample_rate=24000, clip_duration=15):\n",
    "    # Determine the file format\n",
    "    file_format = input_file.split('.')[-1].lower()\n",
    "    \n",
    "    if file_format == 'mp4':\n",
    "        # Load the MP4 file\n",
    "        video = VideoFileClip(input_file)\n",
    "        audio = video.audio\n",
    "        audio.write_audiofile(\"temp_audio.wav\")\n",
    "        audio = AudioSegment.from_file(\"temp_audio.wav\")\n",
    "    else:\n",
    "        # Load the input file (assumed to be WAV)\n",
    "        audio = AudioSegment.from_file(input_file)\n",
    "    \n",
    "    # Convert to mono\n",
    "    audio = audio.set_channels(1)\n",
    "    \n",
    "    # Resample to the target sample rate\n",
    "    audio = audio.set_frame_rate(target_sample_rate)\n",
    "    \n",
    "    # Calculate the start and end times for clipping\n",
    "    start_time = (len(audio) - clip_duration * 1000) // 2\n",
    "    end_time = start_time + clip_duration * 1000\n",
    "    \n",
    "    # Clip the audio\n",
    "    clipped_audio = audio[start_time:end_time]\n",
    "    \n",
    "    # Save the output file\n",
    "    clipped_audio.export(output_file, format=\"wav\")\n",
    "\n",
    "# Paths to the audio files\n",
    "input_file = r\"2024-08-15 21-48-51 (enhanced).wav\"  # or .wav\n",
    "output_file = \"XTTS-v2\\samples\\Eren_test.wav\"\n",
    "\n",
    "# Convert and clip the audio file\n",
    "convert_and_clip_audio(input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-8 (synthesize_audio):\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 761, in run_closure\n",
      "Exception in thread Thread-6 (synthesize_audio):\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\threading.py\", line 1016, in _bootstrap_inner\n",
      "    _threading_Thread_run(self)\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\threading.py\", line 953, in run\n",
      "    self.run()\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 761, in run_closure\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\kalin\\AppData\\Local\\Temp\\ipykernel_10492\\1680889571.py\", line 13, in synthesize_audio\n",
      "    _threading_Thread_run(self)\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\kalin\\AppData\\Local\\Temp\\ipykernel_10492\\1680889571.py\", line 13, in synthesize_audio\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\TTS\\tts\\models\\xtts.py\", line 424, in synthesize\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\TTS\\tts\\models\\xtts.py\", line 424, in synthesize\n",
      "    return self.full_inference(text, speaker_wav, language, **settings)\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\utils\\_contextlib.py\", line 115, in decorate_context\n",
      "    return self.full_inference(text, speaker_wav, language, **settings)\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\utils\\_contextlib.py\", line 115, in decorate_context\n",
      "Exception in thread Thread-10 (synthesize_audio):\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\threading.py\", line 1016, in _bootstrap_inner\n",
      "    return func(*args, **kwargs)\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\TTS\\tts\\models\\xtts.py\", line 493, in full_inference\n",
      "    return func(*args, **kwargs)\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\TTS\\tts\\models\\xtts.py\", line 493, in full_inference\n",
      "    self.run()\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 761, in run_closure\n",
      "    return self.inference(\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\utils\\_contextlib.py\", line 115, in decorate_context\n",
      "    return self.inference(\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\utils\\_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\TTS\\tts\\models\\xtts.py\", line 546, in inference\n",
      "    return func(*args, **kwargs)\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\TTS\\tts\\models\\xtts.py\", line 546, in inference\n",
      "    gpt_codes = self.gpt.generate(\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\TTS\\tts\\layers\\xtts\\gpt.py\", line 589, in generate\n",
      "    _threading_Thread_run(self)\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\threading.py\", line 953, in run\n",
      "    gpt_codes = self.gpt.generate(\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\TTS\\tts\\layers\\xtts\\gpt.py\", line 589, in generate\n",
      "    gen = self.gpt_inference.generate(\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\utils\\_contextlib.py\", line 115, in decorate_context\n",
      "    gen = self.gpt_inference.generate(\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\utils\\_contextlib.py\", line 115, in decorate_context\n",
      "Exception in thread Thread-11 (synthesize_audio):\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\threading.py\", line 1016, in _bootstrap_inner\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\kalin\\AppData\\Local\\Temp\\ipykernel_10492\\1680889571.py\", line 13, in synthesize_audio\n",
      "    return func(*args, **kwargs)\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\transformers\\generation\\utils.py\", line 1622, in generate\n",
      "    return func(*args, **kwargs)\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\transformers\\generation\\utils.py\", line 1622, in generate\n",
      "    self.run()\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 761, in run_closure\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\TTS\\tts\\models\\xtts.py\", line 424, in synthesize\n",
      "Exception in thread Thread-7 (synthesize_audio):\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\threading.py\", line 1016, in _bootstrap_inner\n",
      "    result = self._sample(\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\transformers\\generation\\utils.py\", line 2791, in _sample\n",
      "    result = self._sample(\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\transformers\\generation\\utils.py\", line 2791, in _sample\n",
      "    self.run()\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 761, in run_closure\n",
      "    return self.full_inference(text, speaker_wav, language, **settings)\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\utils\\_contextlib.py\", line 115, in decorate_context\n",
      "    _threading_Thread_run(self)\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\threading.py\", line 953, in run\n",
      "    _threading_Thread_run(self)\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\threading.py\", line 953, in run\n",
      "    return func(*args, **kwargs)\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\TTS\\tts\\models\\xtts.py\", line 493, in full_inference\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\kalin\\AppData\\Local\\Temp\\ipykernel_10492\\1680889571.py\", line 13, in synthesize_audio\n",
      "    return self.inference(\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\utils\\_contextlib.py\", line 115, in decorate_context\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\kalin\\AppData\\Local\\Temp\\ipykernel_10492\\1680889571.py\", line 13, in synthesize_audio\n",
      "    outputs = self(\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1532, in _wrapped_call_impl\n",
      "    return func(*args, **kwargs)\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\TTS\\tts\\models\\xtts.py\", line 546, in inference\n",
      "    outputs = self(\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1532, in _wrapped_call_impl\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\TTS\\tts\\models\\xtts.py\", line 424, in synthesize\n",
      "    gpt_codes = self.gpt.generate(\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\TTS\\tts\\layers\\xtts\\gpt.py\", line 589, in generate\n",
      "Exception in thread Thread-12 (synthesize_audio):\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\threading.py\", line 1016, in _bootstrap_inner\n",
      "    gen = self.gpt_inference.generate(\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\utils\\_contextlib.py\", line 115, in decorate_context\n",
      "    self.run()\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 761, in run_closure\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1541, in _call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1541, in _call_impl\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\TTS\\tts\\models\\xtts.py\", line 424, in synthesize\n",
      "    return self.full_inference(text, speaker_wav, language, **settings)\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\utils\\_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\transformers\\generation\\utils.py\", line 1622, in generate\n",
      "    _threading_Thread_run(self)\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\threading.py\", line 953, in run\n",
      "    result = self._sample(\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\transformers\\generation\\utils.py\", line 2791, in _sample\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\TTS\\tts\\layers\\xtts\\gpt_inference.py\", line 95, in forward\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\TTS\\tts\\layers\\xtts\\gpt_inference.py\", line 95, in forward\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\kalin\\AppData\\Local\\Temp\\ipykernel_10492\\1680889571.py\", line 13, in synthesize_audio\n",
      "    transformer_outputs = self.transformer(\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1532, in _wrapped_call_impl\n",
      "    return self.full_inference(text, speaker_wav, language, **settings)\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\utils\\_contextlib.py\", line 115, in decorate_context\n",
      "    transformer_outputs = self.transformer(\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1532, in _wrapped_call_impl\n",
      "    return func(*args, **kwargs)\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\TTS\\tts\\models\\xtts.py\", line 493, in full_inference\n",
      "    return func(*args, **kwargs)\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\TTS\\tts\\models\\xtts.py\", line 493, in full_inference\n",
      "    outputs = self(\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1532, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1541, in _call_impl\n",
      "    return self.inference(\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\utils\\_contextlib.py\", line 115, in decorate_context\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1541, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py\", line 1119, in forward\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1541, in _call_impl\n",
      "    return self.inference(\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\utils\\_contextlib.py\", line 115, in decorate_context\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\TTS\\tts\\models\\xtts.py\", line 424, in synthesize\n",
      "    return func(*args, **kwargs)\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\TTS\\tts\\models\\xtts.py\", line 546, in inference\n",
      "    return func(*args, **kwargs)\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\TTS\\tts\\models\\xtts.py\", line 546, in inference\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\TTS\\tts\\layers\\xtts\\gpt_inference.py\", line 95, in forward\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py\", line 1070, in forward\n",
      "    return self.full_inference(text, speaker_wav, language, **settings)\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\utils\\_contextlib.py\", line 115, in decorate_context\n",
      "    outputs = block(\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1532, in _wrapped_call_impl\n",
      "    transformer_outputs = self.transformer(\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1532, in _wrapped_call_impl\n",
      "    gpt_codes = self.gpt.generate(\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\TTS\\tts\\layers\\xtts\\gpt.py\", line 589, in generate\n",
      "    return func(*args, **kwargs)\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\TTS\\tts\\models\\xtts.py\", line 493, in full_inference\n",
      "    hidden_states = inputs_embeds + position_embeds\n",
      "RuntimeError: The size of tensor a (123) must match the size of tensor b (60) at non-singleton dimension 1\n",
      "    gpt_codes = self.gpt.generate(\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\TTS\\tts\\layers\\xtts\\gpt.py\", line 589, in generate\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1541, in _call_impl\n",
      "    return self.inference(\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\utils\\_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\TTS\\tts\\models\\xtts.py\", line 546, in inference\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py\", line 1119, in forward\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1541, in _call_impl\n",
      "    gen = self.gpt_inference.generate(\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\utils\\_contextlib.py\", line 115, in decorate_context\n",
      "    outputs = block(\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1532, in _wrapped_call_impl\n",
      "    gen = self.gpt_inference.generate(\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\utils\\_contextlib.py\", line 115, in decorate_context\n",
      "    gpt_codes = self.gpt.generate(\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\TTS\\tts\\layers\\xtts\\gpt.py\", line 589, in generate\n",
      "    return func(*args, **kwargs)\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\transformers\\generation\\utils.py\", line 1622, in generate\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py\", line 617, in forward\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1541, in _call_impl\n",
      "    return func(*args, **kwargs)\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\transformers\\generation\\utils.py\", line 1622, in generate\n",
      "    gen = self.gpt_inference.generate(\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\utils\\_contextlib.py\", line 115, in decorate_context\n",
      "    attn_outputs = self.attn(\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1532, in _wrapped_call_impl\n",
      "    return func(*args, **kwargs)\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\transformers\\generation\\utils.py\", line 1622, in generate\n",
      "    result = self._sample(\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\transformers\\generation\\utils.py\", line 2791, in _sample\n",
      "    result = self._sample(\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\transformers\\generation\\utils.py\", line 2791, in _sample\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py\", line 617, in forward\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1541, in _call_impl\n",
      "    outputs = self(\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1532, in _wrapped_call_impl\n",
      "    result = self._sample(\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\transformers\\generation\\utils.py\", line 2791, in _sample\n",
      "    attn_outputs = self.attn(\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1532, in _wrapped_call_impl\n",
      "    outputs = self(\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1532, in _wrapped_call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py\", line 347, in forward\n",
      "    outputs = self(\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1532, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1541, in _call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1541, in _call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1541, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\TTS\\tts\\layers\\xtts\\gpt_inference.py\", line 95, in forward\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py\", line 347, in forward\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1541, in _call_impl\n",
      "    attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py\", line 234, in _attn\n",
      "    attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py\", line 234, in _attn\n",
      "    transformer_outputs = self.transformer(\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1532, in _wrapped_call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\TTS\\tts\\layers\\xtts\\gpt_inference.py\", line 95, in forward\n",
      "    attn_output = torch.matmul(attn_weights, value)\n",
      "RuntimeError: CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasSgemmStridedBatched( handle, opa, opb, m, n, k, &alpha, a, lda, stridea, b, ldb, strideb, &beta, c, ldc, stridec, num_batches)`\n",
      "    attn_output = torch.matmul(attn_weights, value)\n",
      "RuntimeError: CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasSgemmStridedBatched( handle, opa, opb, m, n, k, &alpha, a, lda, stridea, b, ldb, strideb, &beta, c, ldc, stridec, num_batches)`\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1541, in _call_impl\n",
      "        transformer_outputs = self.transformer(\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1532, in _wrapped_call_impl\n",
      "return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\TTS\\tts\\layers\\xtts\\gpt_inference.py\", line 95, in forward\n",
      "    c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\conv.py:306: UserWarning: Plan failed with a CuDNNError: cuDNN error: CUDNN_STATUS_EXECUTION_FAILED\n",
      "Exception raised from run_conv_plan at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cudnn\\Conv_v8.cpp:375 (most recent call first):\n",
      "00007FFAF736366200007FFAF7363600 c10.dll!c10::Error::Error [<unknown file> @ <unknown line number>]\n",
      "00007FF9DEC3784600007FF9DEC34EB0 torch_cuda.dll!at::native::_fft_r2c_cufft_out [<unknown file> @ <unknown line number>]\n",
      "00007FF9DEC7257000007FF9DEC418C0 torch_cuda.dll!at::native::cudnn_convolution_transpose [<unknown file> @ <unknown line number>]\n",
      "00007FF9DEC7806600007FF9DEC418C0 torch_cuda.dll!at::native::cudnn_convolution_transpose [<unknown file> @ <unknown line number>]\n",
      "00007FF9DEC73A5900007FF9DEC418C0 torch_cuda.dll!at::native::cudnn_convolution_transpose [<unknown file> @ <unknown line number>]\n",
      "00007FF9DEC71EAB00007FF9DEC418C0 torch_cuda.dll!at::native::cudnn_convolution_transpose [<unknown file> @ <unknown line number>]\n",
      "00007FF9DEC411CB00007FF9DEC3FB60 torch_cuda.dll!at::native::cudnn_convolution_add_relu [<unknown file> @ <unknown line number>]\n",
      "00007FF9DEC3FB1B00007FF9DEC3F790 torch_cuda.dll!at::native::cudnn_convolution [<unknown file> @ <unknown line number>]\n",
      "00007FF9E078DD5400007FF9E0772010 torch_cuda.dll!at::cuda::where_outf [<unknown file> @ <unknown line number>]\n",
      "00007FF9E06B6FEA00007FF9E064E5D0 torch_cuda.dll!at::native::_scaled_mm_oution [<unknown file> @ <unknown line number>]\n",
      "00007FF9D7BEEDE600007FF9D7BE1700 torch_cpu.dll!at::_ops::vsplit_int::redispatch [<unknown file> @ <unknown line number>]\n",
      "00007FF9D7C67AF500007FF9D7C67810 torch_cpu.dll!at::_ops::cudnn_convolution::call [<unknown file> @ <unknown line number>]\n",
      "00007FF9D71F729000007FF9D71F6490 torch_cpu.dll!at::native::_convolution [<unknown file> @ <unknown line number>]\n",
      "00007FF9D807995D00007FF9D80789D0 torch_cpu.dll!at::compositeexplicitautograd::view_copy_symint_outf [<unknown file> @ <unknown line number>]\n",
      "00007FF9D804A5DE00007FF9D800D610 torch_cpu.dll!at::compositeexplicitautograd::bucketize_outf [<unknown file> @ <unknown line number>]\n",
      "00007FF9D7780C6400007FF9D7760CD0 torch_cpu.dll!at::TensorMaker::make_tensor [<unknown file> @ <unknown line number>]\n",
      "00007FF9D7821A0200007FF9D7821620 torch_cpu.dll!at::_ops::_convolution::call [<unknown file> @ <unknown line number>]\n",
      "00007FF9D71F646B00007FF9D71F51C0 torch_cpu.dll!at::native::sym_size [<unknown file> @ <unknown line number>]\n",
      "00007FF9D720285B00007FF9D72026F0 torch_cpu.dll!at::native::convolution [<unknown file> @ <unknown line number>]\n",
      "00007FF9D807AE9300007FF9D80789D0 torch_cpu.dll!at::compositeexplicitautograd::view_copy_symint_outf [<unknown file> @ <unknown line number>]\n",
      "00007FF9D804A6EF00007FF9D800D610 torch_cpu.dll!at::compositeexplicitautograd::bucketize_outf [<unknown file> @ <unknown line number>]\n",
      "00007FF9D7780B0000007FF9D7760CD0 torch_cpu.dll!at::TensorMaker::make_tensor [<unknown file> @ <unknown line number>]\n",
      "00007FF9D784DF9600007FF9D784DC60 torch_cpu.dll!at::_ops::convolution::call [<unknown file> @ <unknown line number>]\n",
      "00007FF9D7200C2600007FF9D72008D0 torch_cpu.dll!at::native::conv1d_symint [<unknown file> @ <unknown line number>]\n",
      "00007FF9D820D4D800007FF9D820B0B0 torch_cpu.dll!at::compositeimplicitautograd::where [<unknown file> @ <unknown line number>]\n",
      "00007FF9D81F096000007FF9D81B8970 torch_cpu.dll!at::compositeimplicitautograd::broadcast_to_symint [<unknown file> @ <unknown line number>]\n",
      "00007FF9D77808DB00007FF9D7760CD0 torch_cpu.dll!at::TensorMaker::make_tensor [<unknown file> @ <unknown line number>]\n",
      "00007FF9D7C639BF00007FF9D7C63700 torch_cpu.dll!at::_ops::conv1d::call [<unknown file> @ <unknown line number>]\n",
      "00007FF95ABEC2B900007FF95AA83540 torch_python.dll!THPPointer<THPGenerator>::release [<unknown file> @ <unknown line number>]\n",
      "00007FF95AC421B500007FF95AA83540 torch_python.dll!THPPointer<THPGenerator>::release [<unknown file> @ <unknown line number>]\n",
      "00007FFA9C2E82F600007FFA9C2E7530 python310.dll!PyCFunction_GetFlags [<unknown file> @ <unknown line number>]\n",
      "00007FFA9C2A554C00007FFA9C2A5410 python310.dll!PyObject_MakeTpCall [<unknown file> @ <unknown line number>]\n",
      "00007FFA9C39E6F200007FFA9C39E300 python310.dll!PyEval_GetFuncDesc [<unknown file> @ <unknown line number>]\n",
      "00007FFA9C39AE9F00007FFA9C397A70 python310.dll!PyEval_EvalFrameDefault [<unknown file> @ <unknown line number>]\n",
      "00007FFA9C39CE7B00007FFA9C397A70 python310.dll!PyEval_EvalFrameDefault [<unknown file> @ <unknown line number>]\n",
      "00007FFA9C2A585E00007FFA9C2A5820 python310.dll!PyFunction_Vectorcall [<unknown file> @ <unknown line number>]\n",
      "00007FFA9C2A77D200007FFA9C2A7700 python310.dll!PyMethod_Self [<unknown file> @ <unknown line number>]\n",
      "00007FFA9C3961A900007FFA9C396060 python310.dll!PyOS_URandomNonblock [<unknown file> @ <unknown line number>]\n",
      "00007FFA9C39E6F200007FFA9C39E300 python310.dll!PyEval_GetFuncDesc [<unknown file> @ <unknown line number>]\n",
      "00007FFA9C39AE9F00007FFA9C397A70 python310.dll!PyEval_EvalFrameDefault [<unknown file> @ <unknown line number>]\n",
      "00007FFA9C39CE7B00007FFA9C397A70 python310.dll!PyEval_EvalFrameDefault [<unknown file> @ <unknown line number>]\n",
      "00007FFA9C2A585E00007FFA9C2A5820 python310.dll!PyFunction_Vectorcall [<unknown file> @ <unknown line number>]\n",
      "00007FFA9C2A766900007FFA9C2A7270 python310.dll!PyCell_Set [<unknown file> @ <unknown line number>]\n",
      "00007FFA9C2A78F100007FFA9C2A7700 python310.dll!PyMethod_Self [<unknown file> @ <unknown line number>]\n",
      "00007FFA9C39E8F200007FFA9C39E300 python310.dll!PyEval_GetFuncDesc [<unknown file> @ <unknown line number>]\n",
      "00007FFA9C399D1900007FFA9C397A70 python310.dll!PyEval_EvalFrameDefault [<unknown file> @ <unknown line number>]\n",
      "00007FFA9C39CE7B00007FFA9C397A70 python310.dll!PyEval_EvalFrameDefault [<unknown file> @ <unknown line number>]\n",
      "00007FFA9C2A585E00007FFA9C2A5820 python310.dll!PyFunction_Vectorcall [<unknown file> @ <unknown line number>]\n",
      "00007FFA9C2A766900007FFA9C2A7270 python310.dll!PyCell_Set [<unknown file> @ <unknown line number>]\n",
      "00007FFA9C2A78F100007FFA9C2A7700 python310.dll!PyMethod_Self [<unknown file> @ <unknown line number>]\n",
      "00007FFA9C39E8F200007FFA9C39E300 python310.dll!PyEval_GetFuncDesc [<unknown file> @ <unknown line number>]\n",
      "00007FFA9C399D1900007FFA9C397A70 python310.dll!PyEval_EvalFrameDefault [<unknown file> @ <unknown line number>]\n",
      "00007FFA9C39CE7B00007FFA9C397A70 python310.dll!PyEval_EvalFrameDefault [<unknown file> @ <unknown line number>]\n",
      "00007FFA9C2A585E00007FFA9C2A5820 python310.dll!PyFunction_Vectorcall [<unknown file> @ <unknown line number>]\n",
      "00007FFA9C2A53B400007FFA9C2A52E0 python310.dll!PyObject_FastCallDictTstate [<unknown file> @ <unknown line number>]\n",
      "00007FFA9C2A5AD200007FFA9C2A5A30 python310.dll!PyObject_Call_Prepend [<unknown file> @ <unknown line number>]\n",
      "00007FFA9C30FC0C00007FFA9C30C470 python310.dll!PyType_Ready [<unknown file> @ <unknown line number>]\n",
      "00007FFA9C2A554C00007FFA9C2A5410 python310.dll!PyObject_MakeTpCall [<unknown file> @ <unknown line number>]\n",
      "00007FFA9C39E6F200007FFA9C39E300 python310.dll!PyEval_GetFuncDesc [<unknown file> @ <unknown line number>]\n",
      "00007FFA9C39AE9F00007FFA9C397A70 python310.dll!PyEval_EvalFrameDefault [<unknown file> @ <unknown line number>]\n",
      "00007FFA9C39CE7B00007FFA9C397A70 python310.dll!PyEval_EvalFrameDefault [<unknown file> @ <unknown line number>]\n",
      "00007FFA9C2A585E00007FFA9C2A5820 python310.dll!PyFunction_Vectorcall [<unknown file> @ <unknown line number>]\n",
      "00007FFA9C2A766900007FFA9C2A7270 python310.dll!PyCell_Set [<unknown file> @ <unknown line number>]\n",
      "00007FFA9C2A78F100007FFA9C2A7700 python310.dll!PyMethod_Self [<unknown file> @ <unknown line number>]\n",
      " (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cudnn\\Conv_v8.cpp:921.)\n",
      "  return F.conv1d(input, weight, bias, self.stride,\n",
      "c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\conv.py:306: UserWarning: Plan failed with a CuDNNError: cuDNN error: CUDNN_STATUS_EXECUTION_FAILED\n",
      "Exception raised from run_conv_plan at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cudnn\\Conv_v8.cpp:375 (most recent call first):\n",
      "00007FFAF736366200007FFAF7363600 c10.dll!c10::Error::Error [<unknown file> @ <unknown line number>]\n",
      "00007FF9DEC3784600007FF9DEC34EB0 torch_cuda.dll!at::native::_fft_r2c_cufft_out [<unknown file> @ <unknown line number>]\n",
      "00007FF9DEC7257000007FF9DEC418C0 torch_cuda.dll!at::native::cudnn_convolution_transpose [<unknown file> @ <unknown line number>]\n",
      "00007FF9DEC7806600007FF9DEC418C0 torch_cuda.dll!at::native::cudnn_convolution_transpose [<unknown file> @ <unknown line number>]\n",
      "00007FF9DEC73A5900007FF9DEC418C0 torch_cuda.dll!at::native::cudnn_convolution_transpose [<unknown file> @ <unknown line number>]\n",
      "00007FF9DEC71EAB00007FF9DEC418C0 torch_cuda.dll!at::native::cudnn_convolution_transpose [<unknown file> @ <unknown line number>]\n",
      "00007FF9DEC411CB00007FF9DEC3FB60 torch_cuda.dll!at::native::cudnn_convolution_add_relu [<unknown file> @ <unknown line number>]\n",
      "00007FF9DEC3FB1B00007FF9DEC3F790 torch_cuda.dll!at::native::cudnn_convolution [<unknown file> @ <unknown line number>]\n",
      "00007FF9E078DD5400007FF9E0772010 torch_cuda.dll!at::cuda::where_outf [<unknown file> @ <unknown line number>]\n",
      "00007FF9E06B6FEA00007FF9E064E5D0 torch_cuda.dll!at::cuda::bucketize_outf [<unknown file> @ <unknown line number>]\n",
      "00007FF9D7BEEDE600007FF9D7BE1700 torch_cpu.dll!at::_ops::vsplit_int::redispatch [<unknown file> @ <unknown line number>]\n",
      "00007FF9D7C67AF500007FF9D7C67810 torch_cpu.dll!at::_ops::cudnn_convolution::call [<unknown file> @ <unknown line number>]\n",
      "00007FF9D71F729000007FF9D71F6490 torch_cpu.dll!at::native::_convolution [<unknown file> @ <unknown line number>]\n",
      "00007FF9D807995D00007FF9D80789D0 torch_cpu.dll!at::compositeexplicitautograd::view_copy_symint_outf [<unknown file> @ <unknown line number>]\n",
      "00007FF9D804A5DE00007FF9D800D610 torch_cpu.dll!at::compositeexplicitautograd::bucketize_outf [<unknown file> @ <unknown line number>]\n",
      "00007FF9D7780C6400007FF9D7760CD0 torch_cpu.dll!at::TensorMaker::make_tensor [<unknown file> @ <unknown line number>]\n",
      "00007FF9D7821A0200007FF9D7821620 torch_cpu.dll!at::_ops::_convolution::call [<unknown file> @ <unknown line number>]\n",
      "00007FF9D71F646B00007FF9D71F51C0 torch_cpu.dll!at::native::sym_size [<unknown file> @ <unknown line number>]\n",
      "00007FF9D720285B00007FF9D72026F0 torch_cpu.dll!at::native::convolution [<unknown file> @ <unknown line number>]\n",
      "00007FF9D807AE9300007FF9D80789D0 torch_cpu.dll!at::compositeexplicitautograd::view_copy_symint_outf [<unknown file> @ <unknown line number>]\n",
      "00007FF9D804A6EF00007FF9D800D610 torch_cpu.dll!at::compositeexplicitautograd::bucketize_outf [<unknown file> @ <unknown line number>]\n",
      "00007FF9D7780B0000007FF9D7760CD0 torch_cpu.dll!at::TensorMaker::make_tensor [<unknown file> @ <unknown line number>]\n",
      "00007FF9D784DF9600007FF9D784DC60 torch_cpu.dll!at::_ops::convolution::call [<unknown file> @ <unknown line number>]\n",
      "00007FF9D7200C2600007FF9D72008D0 torch_cpu.dll!at::native::conv1d_symint [<unknown file> @ <unknown line number>]\n",
      "00007FF9D820D4D800007FF9D820B0B0 torch_cpu.dll!at::compositeimplicitautograd::where [<unknown file> @ <unknown line number>]\n",
      "00007FF9D81F096000007FF9D81B8970 torch_cpu.dll!at::compositeimplicitautograd::broadcast_to_symint [<unknown file> @ <unknown line number>]\n",
      "00007FF9D77808DB00007FF9D7760CD0 torch_cpu.dll!at::TensorMaker::make_tensor [<unknown file> @ <unknown line number>]\n",
      "00007FF9D7C639BF00007FF9D7C63700 torch_cpu.dll!at::_ops::conv1d::call [<unknown file> @ <unknown line number>]\n",
      "00007FF95ABEC2B900007FF95AA83540 torch_python.dll!THPPointer<THPGenerator>::release [<unknown file> @ <unknown line number>]\n",
      "00007FF95AC421B500007FF95AA83540 torch_python.dll!THPPointer<THPGenerator>::release [<unknown file> @ <unknown line number>]\n",
      "00007FFA9C2E82F600007FFA9C2E7530 python310.dll!PyCFunction_GetFlags [<unknown file> @ <unknown line number>]\n",
      "00007FFA9C2A554C00007FFA9C2A5410 python310.dll!PyObject_MakeTpCall [<unknown file> @ <unknown line number>]\n",
      "00007FFA9C39E6F200007FFA9C39E300 python310.dll!PyEval_GetFuncDesc [<unknown file> @ <unknown line number>]\n",
      "00007FFA9C39AE9F00007FFA9C397A70 python310.dll!PyEval_EvalFrameDefault [<unknown file> @ <unknown line number>]\n",
      "00007FFA9C39CE7B00007FFA9C397A70 python310.dll!PyEval_EvalFrameDefault [<unknown file> @ <unknown line number>]\n",
      "00007FFA9C2A585E00007FFA9C2A5820 python310.dll!PyFunction_Vectorcall [<unknown file> @ <unknown line number>]\n",
      "00007FFA9C2A77D200007FFA9C2A7700 python310.dll!PyMethod_Self [<unknown file> @ <unknown line number>]\n",
      "00007FFA9C3961A900007FFA9C396060 python310.dll!PyOS_URandomNonblock [<unknown file> @ <unknown line number>]\n",
      "00007FFA9C39E6F200007FFA9C39E300 python310.dll!PyEval_GetFuncDesc [<unknown file> @ <unknown line number>]\n",
      "00007FFA9C39AE9F00007FFA9C397A70 python310.dll!PyEval_EvalFrameDefault [<unknown file> @ <unknown line number>]\n",
      "00007FFA9C39CE7B00007FFA9C397A70 python310.dll!PyEval_EvalFrameDefault [<unknown file> @ <unknown line number>]\n",
      "00007FFA9C2A585E00007FFA9C2A5820 python310.dll!PyFunction_Vectorcall [<unknown file> @ <unknown line number>]\n",
      "00007FFA9C2A766900007FFA9C2A7270 python310.dll!PyCell_Set [<unknown file> @ <unknown line number>]\n",
      "00007FFA9C2A78F100007FFA9C2A7700 python310.dll!PyMethod_Self [<unknown file> @ <unknown line number>]\n",
      "00007FFA9C39E8F200007FFA9C39E300 python310.dll!PyEval_GetFuncDesc [<unknown file> @ <unknown line number>]\n",
      "00007FFA9C399D1900007FFA9C397A70 python310.dll!PyEval_EvalFrameDefault [<unknown file> @ <unknown line number>]\n",
      "00007FFA9C39CE7B00007FFA9C397A70 python310.dll!PyEval_EvalFrameDefault [<unknown file> @ <unknown line number>]\n",
      "00007FFA9C2A585E00007FFA9C2A5820 python310.dll!PyFunction_Vectorcall [<unknown file> @ <unknown line number>]\n",
      "00007FFA9C2A766900007FFA9C2A7270 python310.dll!PyCell_Set [<unknown file> @ <unknown line number>]\n",
      "00007FFA9C2A78F100007FFA9C2A7700 python310.dll!PyMethod_Self [<unknown file> @ <unknown line number>]\n",
      "00007FFA9C39E8F200007FFA9C39E300 python310.dll!PyEval_GetFuncDesc [<unknown file> @ <unknown line number>]\n",
      "00007FFA9C399D1900007FFA9C397A70 python310.dll!PyEval_EvalFrameDefault [<unknown file> @ <unknown line number>]\n",
      "00007FFA9C39CE7B00007FFA9C397A70 python310.dll!PyEval_EvalFrameDefault [<unknown file> @ <unknown line number>]\n",
      "00007FFA9C2A585E00007FFA9C2A5820 python310.dll!PyFunction_Vectorcall [<unknown file> @ <unknown line number>]\n",
      "00007FFA9C2A53B400007FFA9C2A52E0 python310.dll!PyObject_FastCallDictTstate [<unknown file> @ <unknown line number>]\n",
      "00007FFA9C2A5AD200007FFA9C2A5A30 python310.dll!PyObject_Call_Prepend [<unknown file> @ <unknown line number>]\n",
      "00007FFA9C30FC0C00007FFA9C30C470 python310.dll!PyType_Ready [<unknown file> @ <unknown line number>]\n",
      "00007FFA9C2A554C00007FFA9C2A5410 python310.dll!PyObject_MakeTpCall [<unknown file> @ <unknown line number>]\n",
      "00007FFA9C39E6F200007FFA9C39E300 python310.dll!PyEval_GetFuncDesc [<unknown file> @ <unknown line number>]\n",
      "00007FFA9C39AE9F00007FFA9C397A70 python310.dll!PyEval_EvalFrameDefault [<unknown file> @ <unknown line number>]\n",
      "00007FFA9C39CE7B00007FFA9C397A70 python310.dll!PyEval_EvalFrameDefault [<unknown file> @ <unknown line number>]\n",
      "00007FFA9C2A585E00007FFA9C2A5820 python310.dll!PyFunction_Vectorcall [<unknown file> @ <unknown line number>]\n",
      "00007FFA9C2A766900007FFA9C2A7270 python310.dll!PyCell_Set [<unknown file> @ <unknown line number>]\n",
      "00007FFA9C2A78F100007FFA9C2A7700 python310.dll!PyMethod_Self [<unknown file> @ <unknown line number>]\n",
      " (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cudnn\\Conv_v8.cpp:921.)\n",
      "  return F.conv1d(input, weight, bias, self.stride,\n",
      "c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\conv.py:306: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cudnn\\Conv_v8.cpp:919.)\n",
      "  return F.conv1d(input, weight, bias, self.stride,\n",
      "c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\conv.py:306: UserWarning: Plan failed with a CuDNNError: cuDNN error: CUDNN_STATUS_EXECUTION_FAILED\n",
      "Exception raised from run_conv_plan at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cudnn\\Conv_v8.cpp:375 (most recent call first):\n",
      "00007FFAF736366200007FFAF7363600 c10.dll!c10::Error::Error [<unknown file> @ <unknown line number>]\n",
      "00007FF9DEC3784600007FF9DEC34EB0 torch_cuda.dll!at::native::_fft_r2c_cufft_out [<unknown file> @ <unknown line number>]\n",
      "00007FF9DEC7257000007FF9DEC418C0 torch_cuda.dll!at::native::cudnn_convolution_transpose [<unknown file> @ <unknown line number>]\n",
      "00007FF9DEC7806600007FF9DEC418C0 torch_cuda.dll!at::native::cudnn_convolution_transpose [<unknown file> @ <unknown line number>]\n",
      "00007FF9DEC73DEE00007FF9DEC418C0 torch_cuda.dll!at::native::cudnn_convolution_transpose [<unknown file> @ <unknown line number>]\n",
      "00007FF9DEC71EAB00007FF9DEC418C0 torch_cuda.dll!at::native::cudnn_convolution_transpose [<unknown file> @ <unknown line number>]\n",
      "00007FF9DEC411CB00007FF9DEC3FB60 torch_cuda.dll!at::native::cudnn_convolution_add_relu [<unknown file> @ <unknown line number>]\n",
      "00007FF9DEC3FB1B00007FF9DEC3F790 torch_cuda.dll!at::native::cudnn_convolution [<unknown file> @ <unknown line number>]\n",
      "00007FF9E078DD5400007FF9E0772010 torch_cuda.dll!at::cuda::where_outf [<unknown file> @ <unknown line number>]\n",
      "00007FF9E06B6FEA00007FF9E064E5D0 torch_cuda.dll!at::cuda::bucketize_outf [<unknown file> @ <unknown line number>]\n",
      "00007FF9D7BEEDE600007FF9D7BE1700 torch_cpu.dll!at::_ops::vsplit_int::redispatch [<unknown file> @ <unknown line number>]\n",
      "00007FF9D7C67AF500007FF9D7C67810 torch_cpu.dll!at::_ops::cudnn_convolution::call [<unknown file> @ <unknown line number>]\n",
      "00007FF9D71F729000007FF9D71F6490 torch_cpu.dll!at::native::_convolution [<unknown file> @ <unknown line number>]\n",
      "00007FF9D807995D00007FF9D80789D0 torch_cpu.dll!at::compositeexplicitautograd::view_copy_symint_outf [<unknown file> @ <unknown line number>]\n",
      "00007FF9D804A5DE00007FF9D800D610 torch_cpu.dll!at::compositeexplicitautograd::bucketize_outf [<unknown file> @ <unknown line number>]\n",
      "00007FF9D7780C6400007FF9D7760CD0 torch_cpu.dll!at::TensorMaker::make_tensor [<unknown file> @ <unknown line number>]\n",
      "00007FF9D7821A0200007FF9D7821620 torch_cpu.dll!at::_ops::_convolution::call [<unknown file> @ <unknown line number>]\n",
      "00007FF9D71F646B00007FF9D71F51C0 torch_cpu.dll!at::native::sym_size [<unknown file> @ <unknown line number>]\n",
      "00007FF9D720285B00007FF9D72026F0 torch_cpu.dll!at::native::convolution [<unknown file> @ <unknown line number>]\n",
      "00007FF9D807AE9300007FF9D80789D0 torch_cpu.dll!at::compositeexplicitautograd::view_copy_symint_outf [<unknown file> @ <unknown line number>]\n",
      "00007FF9D804A6EF00007FF9D800D610 torch_cpu.dll!at::compositeexplicitautograd::bucketize_outf [<unknown file> @ <unknown line number>]\n",
      "00007FF9D7780B0000007FF9D7760CD0 torch_cpu.dll!at::TensorMaker::make_tensor [<unknown file> @ <unknown line number>]\n",
      "00007FF9D784DF9600007FF9D784DC60 torch_cpu.dll!at::_ops::convolution::call [<unknown file> @ <unknown line number>]\n",
      "00007FF9D7200C2600007FF9D72008D0 torch_cpu.dll!at::native::conv1d_symint [<unknown file> @ <unknown line number>]\n",
      "00007FF9D820D4D800007FF9D820B0B0 torch_cpu.dll!at::compositeimplicitautograd::where [<unknown file> @ <unknown line number>]\n",
      "00007FF9D81F096000007FF9D81B8970 torch_cpu.dll!at::compositeimplicitautograd::broadcast_to_symint [<unknown file> @ <unknown line number>]\n",
      "00007FF9D77808DB00007FF9D7760CD0 torch_cpu.dll!at::TensorMaker::make_tensor [<unknown file> @ <unknown line number>]\n",
      "00007FF9D7C639BF00007FF9D7C63700 torch_cpu.dll!at::_ops::conv1d::call [<unknown file> @ <unknown line number>]\n",
      "00007FF95ABEC2B900007FF95AA83540 torch_python.dll!THPPointer<THPGenerator>::release [<unknown file> @ <unknown line number>]\n",
      "00007FF95AC421B500007FF95AA83540 torch_python.dll!THPPointer<THPGenerator>::release [<unknown file> @ <unknown line number>]\n",
      "00007FFA9C2E82F600007FFA9C2E7530 python310.dll!PyCFunction_GetFlags [<unknown file> @ <unknown line number>]\n",
      "00007FFA9C2A554C00007FFA9C2A5410 python310.dll!PyObject_MakeTpCall [<unknown file> @ <unknown line number>]\n",
      "00007FFA9C39E6F200007FFA9C39E300 python310.dll!PyEval_GetFuncDesc [<unknown file> @ <unknown line number>]\n",
      "00007FFA9C39AE9F00007FFA9C397A70 python310.dll!PyEval_EvalFrameDefault [<unknown file> @ <unknown line number>]\n",
      "00007FFA9C39CE7B00007FFA9C397A70 python310.dll!PyEval_EvalFrameDefault [<unknown file> @ <unknown line number>]\n",
      "00007FFA9C2A585E00007FFA9C2A5820 python310.dll!PyFunction_Vectorcall [<unknown file> @ <unknown line number>]\n",
      "00007FFA9C2A77D200007FFA9C2A7700 python310.dll!PyMethod_Self [<unknown file> @ <unknown line number>]\n",
      "00007FFA9C3961A900007FFA9C396060 python310.dll!PyOS_URandomNonblock [<unknown file> @ <unknown line number>]\n",
      "00007FFA9C39E6F200007FFA9C39E300 python310.dll!PyEval_GetFuncDesc [<unknown file> @ <unknown line number>]\n",
      "00007FFA9C39AE9F00007FFA9C397A70 python310.dll!PyEval_EvalFrameDefault [<unknown file> @ <unknown line number>]\n",
      "00007FFA9C39CE7B00007FFA9C397A70 python310.dll!PyEval_EvalFrameDefault [<unknown file> @ <unknown line number>]\n",
      "00007FFA9C2A585E00007FFA9C2A5820 python310.dll!PyFunction_Vectorcall [<unknown file> @ <unknown line number>]\n",
      "00007FFA9C2A766900007FFA9C2A7270 python310.dll!PyCell_Set [<unknown file> @ <unknown line number>]\n",
      "00007FFA9C2A78F100007FFA9C2A7700 python310.dll!PyMethod_Self [<unknown file> @ <unknown line number>]\n",
      "00007FFA9C39E8F200007FFA9C39E300 python310.dll!PyEval_GetFuncDesc [<unknown file> @ <unknown line number>]\n",
      "00007FFA9C399D1900007FFA9C397A70 python310.dll!PyEval_EvalFrameDefault [<unknown file> @ <unknown line number>]\n",
      "00007FFA9C39CE7B00007FFA9C397A70 python310.dll!PyEval_EvalFrameDefault [<unknown file> @ <unknown line number>]\n",
      "00007FFA9C2A585E00007FFA9C2A5820 python310.dll!PyFunction_Vectorcall [<unknown file> @ <unknown line number>]\n",
      "00007FFA9C2A766900007FFA9C2A7270 python310.dll!PyCell_Set [<unknown file> @ <unknown line number>]\n",
      "00007FFA9C2A78F100007FFA9C2A7700 python310.dll!PyMethod_Self [<unknown file> @ <unknown line number>]\n",
      "00007FFA9C39E8F200007FFA9C39E300 python310.dll!PyEval_GetFuncDesc [<unknown file> @ <unknown line number>]\n",
      "00007FFA9C399D1900007FFA9C397A70 python310.dll!PyEval_EvalFrameDefault [<unknown file> @ <unknown line number>]\n",
      "00007FFA9C39CE7B00007FFA9C397A70 python310.dll!PyEval_EvalFrameDefault [<unknown file> @ <unknown line number>]\n",
      "00007FFA9C2A585E00007FFA9C2A5820 python310.dll!PyFunction_Vectorcall [<unknown file> @ <unknown line number>]\n",
      "00007FFA9C2A53B400007FFA9C2A52E0 python310.dll!PyObject_FastCallDictTstate [<unknown file> @ <unknown line number>]\n",
      "00007FFA9C2A5AD200007FFA9C2A5A30 python310.dll!PyObject_Call_Prepend [<unknown file> @ <unknown line number>]\n",
      "00007FFA9C30FC0C00007FFA9C30C470 python310.dll!PyType_Ready [<unknown file> @ <unknown line number>]\n",
      "00007FFA9C2A554C00007FFA9C2A5410 python310.dll!PyObject_MakeTpCall [<unknown file> @ <unknown line number>]\n",
      "00007FFA9C39E6F200007FFA9C39E300 python310.dll!PyEval_GetFuncDesc [<unknown file> @ <unknown line number>]\n",
      "00007FFA9C39AE9F00007FFA9C397A70 python310.dll!PyEval_EvalFrameDefault [<unknown file> @ <unknown line number>]\n",
      "00007FFA9C39CE7B00007FFA9C397A70 python310.dll!PyEval_EvalFrameDefault [<unknown file> @ <unknown line number>]\n",
      "00007FFA9C2A585E00007FFA9C2A5820 python310.dll!PyFunction_Vectorcall [<unknown file> @ <unknown line number>]\n",
      "00007FFA9C2A766900007FFA9C2A7270 python310.dll!PyCell_Set [<unknown file> @ <unknown line number>]\n",
      "00007FFA9C2A78F100007FFA9C2A7700 python310.dll!PyMethod_Self [<unknown file> @ <unknown line number>]\n",
      " (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cudnn\\Conv_v8.cpp:921.)\n",
      "  return F.conv1d(input, weight, bias, self.stride,\n",
      "Exception in thread Thread-9 (synthesize_audio):\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\threading.py\", line 1016, in _bootstrap_inner\n",
      "transformer_outputs = self.transformer(\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1532, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1541, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py\", line 1119, in forward\n",
      "    self.run()\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 761, in run_closure\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1541, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py\", line 1119, in forward\n",
      "    outputs = block(\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1532, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1541, in _call_impl\n",
      "    _threading_Thread_run(self)\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\threading.py\", line 953, in run\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py\", line 1119, in forward\n",
      "    outputs = block(\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1532, in _wrapped_call_impl\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\kalin\\AppData\\Local\\Temp\\ipykernel_10492\\1680889571.py\", line 13, in synthesize_audio\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py\", line 654, in forward\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1541, in _call_impl\n",
      "    feed_forward_hidden_states = self.mlp(hidden_states)\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1532, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1541, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py\", line 654, in forward\n",
      "    outputs = block(\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1532, in _wrapped_call_impl\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\TTS\\tts\\models\\xtts.py\", line 424, in synthesize\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py\", line 574, in forward\n",
      "    feed_forward_hidden_states = self.mlp(hidden_states)\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1532, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1541, in _call_impl\n",
      "    hidden_states = self.c_fc(hidden_states)\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1532, in _wrapped_call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py\", line 617, in forward\n",
      "    return self.full_inference(text, speaker_wav, language, **settings)\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\utils\\_contextlib.py\", line 115, in decorate_context\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1541, in _call_impl\n",
      "    attn_outputs = self.attn(\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1532, in _wrapped_call_impl\n",
      "    return func(*args, **kwargs)\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\TTS\\tts\\models\\xtts.py\", line 485, in full_inference\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1541, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py\", line 576, in forward\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1541, in _call_impl\n",
      "    (gpt_cond_latent, speaker_embedding) = self.get_conditioning_latents(\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\utils\\_contextlib.py\", line 115, in decorate_context\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\transformers\\pytorch_utils.py\", line 103, in forward\n",
      "    x = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight)\n",
      "RuntimeError: CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasSgemm( handle, opa, opb, m, n, k, &alpha, a, lda, b, ldb, &beta, c, ldc)`\n",
      "    return func(*args, **kwargs)\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\TTS\\tts\\models\\xtts.py\", line 375, in get_conditioning_latents\n",
      "    hidden_states = self.c_proj(hidden_states)\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1532, in _wrapped_call_impl\n",
      "        gpt_cond_latents = self.get_gpt_cond_latents(\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\utils\\_contextlib.py\", line 115, in decorate_context\n",
      "return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py\", line 328, in forward\n",
      "    return func(*args, **kwargs)\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\TTS\\tts\\models\\xtts.py\", line 297, in get_gpt_cond_latents\n",
      "    query, key, value = self.c_attn(hidden_states).split(self.split_size, dim=2)\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1532, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1541, in _call_impl\n",
      "    style_emb = self.gpt.get_style_emb(mel_chunk.to(self.device), None)\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\TTS\\tts\\layers\\xtts\\gpt.py\", line 361, in get_style_emb\n",
      "    conds = self.conditioning_encoder(cond_input)  # (b, d, s)\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1532, in _wrapped_call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\transformers\\pytorch_utils.py\", line 103, in forward\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1541, in _call_impl\n",
      "    x = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight)\n",
      "RuntimeError: CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasSgemm( handle, opa, opb, m, n, k, &alpha, a, lda, b, ldb, &beta, c, ldc)`\n",
      "        return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\transformers\\pytorch_utils.py\", line 103, in forward\n",
      "return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1541, in _call_impl\n",
      "    x = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight)\n",
      "RuntimeError: CUDA error: device-side assert triggered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\TTS\\tts\\layers\\xtts\\latent_encoder.py\", line 140, in forward\n",
      "    h = self.attn(h)\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1532, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1541, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\container.py\", line 217, in forward\n",
      "    input = module(input)\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1532, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1541, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\TTS\\tts\\layers\\xtts\\latent_encoder.py\", line 112, in forward\n",
      "    qkv = self.qkv(x)\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1532, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1541, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\conv.py\", line 310, in forward\n",
      "    return self._conv_forward(input, self.weight, self.bias)\n",
      "  File \"c:\\Users\\kalin\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\conv.py\", line 306, in _conv_forward\n",
      "    return F.conv1d(input, weight, bias, self.stride,\n",
      "RuntimeError: GET was unable to find an engine to execute this computation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total generation time: 1.1605887413024902 seconds\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import queue\n",
    "from pydub import AudioSegment\n",
    "from pydub.playback import play\n",
    "import soundfile as sf\n",
    "from TTS.tts.configs.xtts_config import XttsConfig\n",
    "from TTS.tts.models.xtts import Xtts\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Function to synthesize audio for a line of text\n",
    "def synthesize_audio(line, config, model, index, audio_queue):\n",
    "    outputs = model.synthesize(\n",
    "        text=line,\n",
    "        config=config,\n",
    "        speaker_wav=\"XTTS-v2\\\\samples\\\\morgan_freeman.wav\",\n",
    "        gpt_cond_len=30,\n",
    "        language=\"en\",\n",
    "        speed=1.1\n",
    "    )\n",
    "    wav_data = outputs['wav']\n",
    "    temp_output_file = f'temp_output_{index}.wav'\n",
    "    sf.write(temp_output_file, wav_data, 22050)  # Assuming the sample rate is 22050 Hz\n",
    "    audio_queue.put(temp_output_file)\n",
    "\n",
    "# Function to play audio from the queue\n",
    "def play_audio(audio_queue):\n",
    "    while True:\n",
    "        audio_file = audio_queue.get()\n",
    "        if audio_file is None:\n",
    "            break\n",
    "        audio_segment = AudioSegment.from_wav(audio_file)\n",
    "        play(audio_segment)\n",
    "        os.remove(audio_file)  # Clean up the temporary file\n",
    "        audio_queue.task_done()\n",
    "\n",
    "# Split the text into lines\n",
    "lines = text.strip().split('\\n')\n",
    "\n",
    "# Load the TTS model\n",
    "config = XttsConfig()\n",
    "config.load_json(\"XTTS-v2\\\\config.json\")\n",
    "model = Xtts.init_from_config(config)\n",
    "model.load_checkpoint(config, checkpoint_dir=\"XTTS-v2\", eval=True)\n",
    "model.cuda()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Create a queue to hold the audio files\n",
    "audio_queue = queue.Queue()\n",
    "\n",
    "# Start the audio playback thread\n",
    "playback_thread = threading.Thread(target=play_audio, args=(audio_queue,))\n",
    "playback_thread.start()\n",
    "\n",
    "# Synthesize audio for each line in separate threads\n",
    "threads = []\n",
    "for index, line in enumerate(lines):\n",
    "    thread = threading.Thread(target=synthesize_audio, args=(line, config, model, index, audio_queue))\n",
    "    threads.append(thread)\n",
    "    thread.start()\n",
    "\n",
    "# Wait for all synthesis threads to finish\n",
    "for thread in threads:\n",
    "    thread.join()\n",
    "\n",
    "# Signal the playback thread to stop\n",
    "audio_queue.put(None)\n",
    "playback_thread.join()\n",
    "\n",
    "end_time = time.time()\n",
    "generation_time = end_time - start_time\n",
    "\n",
    "# Print the generation time\n",
    "print(f\"Total generation time: {generation_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\" Hello. My name is Aaron Kalinsatsilio-Glu. I build meaningful experiences. Let's chat. Besides coding, I enjoy gaming and cooking. Recently, I've been delving into machine learning to make a lasting impact. Alongside my expertise in 3D modeling, animation, software development, and AI, I possess strong leadership and problem-solving skills, refined through innovative projects and AI-driven educational tools development. My leadership, problem-solving,\"]\n"
     ]
    }
   ],
   "source": [
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "import librosa\n",
    "import torch\n",
    "\n",
    "# Load model and processor\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")\n",
    "model.config.forced_decoder_ids = None\n",
    "\n",
    "# Load the MP3 file\n",
    "file_path = \"output_combined - Copy.mp3\"\n",
    "audio_array, original_sampling_rate = librosa.load(file_path, sr=None)\n",
    "\n",
    "# Resample the audio to 16000 Hz\n",
    "target_sampling_rate = 16000\n",
    "if original_sampling_rate != target_sampling_rate:\n",
    "    audio_array = librosa.resample(audio_array, orig_sr=original_sampling_rate, target_sr=target_sampling_rate)\n",
    "\n",
    "# Process the audio file to extract input features\n",
    "input_features = processor(audio_array, sampling_rate=target_sampling_rate, return_tensors=\"pt\").input_features\n",
    "\n",
    "# Generate token IDs\n",
    "predicted_ids = model.generate(input_features)\n",
    "\n",
    "# Decode token IDs to text\n",
    "transcription_with_special_tokens = processor.batch_decode(predicted_ids, skip_special_tokens=False)\n",
    "transcription_without_special_tokens = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n",
    "\n",
    "print(transcription_without_special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hallo, mein Name ist Ehrn und Ehe und ich mag Pizza. Aber ich habe auch andere Interessen wie zum Beispiel Tic Tac To zu spielen. Wann du nicht leidest, magst du die Zeit für die Ehe frei. Ich liebe den Senderismus, in den Mountains und den naturalen Senderes zu verspielen. Eine andere Passion, die ich in der Technologie habe. Ich bin immer exakt am letzten Abenteuer in die Art und Weise. In der Robotik und in der Exploration. Je quoi, wie die Technologie, alle Potentialen, die Resorten, die Nombro, Problemen, Mondio und der Melioré, unsere Qualität. In meiner Freizeitspiele, ich auch gerne Videospiele. Einige meiner Lieblingsspiele sind Strategiespiele wie Zivilisation und Echtzeit-Strategiespiele wie Starcraft. Ich finde, dass diese Spiele meinen Geist herausfordern und mir helfen, strategisches Denken zu entwickeln. Außerdem machen sie viel Spaß, mit Freunden und Familie, zu spielen.\n"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "\n",
    "model = whisper.load_model(\"small\")\n",
    "result = model.transcribe(\"output_combined.wav\")\n",
    "print(result[\"text\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
